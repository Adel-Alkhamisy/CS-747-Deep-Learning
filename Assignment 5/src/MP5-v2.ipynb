{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gym pyvirtualdisplay\n",
    "# !sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade setuptools --user\n",
    "# !pip3 install ez_setup \n",
    "# !pip3 install gym[atari] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -q gym[atari]\n",
    "# !pip install -q autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/aalkhami/miniconda3/envs/myenv/lib/python3.9/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/aalkhami/miniconda3/envs/myenv/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/aalkhami/miniconda3/envs/myenv/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/aalkhami/miniconda3/envs/myenv/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:137: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'tuple'>\u001b[0m\n",
      "  logger.warn(\n",
      "/home/aalkhami/miniconda3/envs/myenv/lib/python3.9/site-packages/gym/spaces/box.py:226: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  logger.warn(\"Casting input x to numpy array.\")\n",
      "/home/aalkhami/miniconda3/envs/myenv/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:167: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space with exception: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space with exception: {e}\")\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Breakout-v5')\n",
    "state = env.reset()\n",
    "#print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('BreakoutDeterministic-v4')\n",
    "# state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aalkhami/miniconda3/envs/myenv/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1008910/2281050267.py:21: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 2.0   memory length: 218   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 2.0\n",
      "episode: 1   score: 0.0   memory length: 341   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.0\n",
      "episode: 2   score: 2.0   memory length: 557   epsilon: 1.0    steps: 216    lr: 1e-05     evaluation reward: 1.3333333333333333\n",
      "episode: 3   score: 1.0   memory length: 711   epsilon: 1.0    steps: 154    lr: 1e-05     evaluation reward: 1.25\n",
      "episode: 4   score: 2.0   memory length: 909   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.4\n",
      "episode: 5   score: 0.0   memory length: 1032   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.1666666666666667\n",
      "episode: 6   score: 2.0   memory length: 1250   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.2857142857142858\n",
      "episode: 7   score: 5.0   memory length: 1563   epsilon: 1.0    steps: 313    lr: 1e-05     evaluation reward: 1.75\n",
      "episode: 8   score: 3.0   memory length: 1808   epsilon: 1.0    steps: 245    lr: 1e-05     evaluation reward: 1.8888888888888888\n",
      "episode: 9   score: 0.0   memory length: 1931   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.7\n",
      "episode: 10   score: 1.0   memory length: 2100   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.6363636363636365\n",
      "episode: 11   score: 0.0   memory length: 2223   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.5\n",
      "episode: 12   score: 0.0   memory length: 2345   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.3846153846153846\n",
      "episode: 13   score: 4.0   memory length: 2623   epsilon: 1.0    steps: 278    lr: 1e-05     evaluation reward: 1.5714285714285714\n",
      "episode: 14   score: 0.0   memory length: 2746   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4666666666666666\n",
      "episode: 15   score: 4.0   memory length: 3021   epsilon: 1.0    steps: 275    lr: 1e-05     evaluation reward: 1.625\n",
      "episode: 16   score: 4.0   memory length: 3316   epsilon: 1.0    steps: 295    lr: 1e-05     evaluation reward: 1.7647058823529411\n",
      "episode: 17   score: 2.0   memory length: 3513   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.7777777777777777\n",
      "episode: 18   score: 4.0   memory length: 3786   epsilon: 1.0    steps: 273    lr: 1e-05     evaluation reward: 1.894736842105263\n",
      "episode: 19   score: 3.0   memory length: 4032   epsilon: 1.0    steps: 246    lr: 1e-05     evaluation reward: 1.95\n",
      "episode: 20   score: 3.0   memory length: 4297   epsilon: 1.0    steps: 265    lr: 1e-05     evaluation reward: 2.0\n",
      "episode: 21   score: 2.0   memory length: 4495   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 2.0\n",
      "episode: 22   score: 0.0   memory length: 4617   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.9130434782608696\n",
      "episode: 23   score: 2.0   memory length: 4817   epsilon: 1.0    steps: 200    lr: 1e-05     evaluation reward: 1.9166666666666667\n",
      "episode: 24   score: 1.0   memory length: 4968   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.88\n",
      "episode: 25   score: 5.0   memory length: 5291   epsilon: 1.0    steps: 323    lr: 1e-05     evaluation reward: 2.0\n",
      "episode: 26   score: 2.0   memory length: 5489   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 2.0\n",
      "episode: 27   score: 0.0   memory length: 5611   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.9285714285714286\n",
      "episode: 28   score: 3.0   memory length: 5876   epsilon: 1.0    steps: 265    lr: 1e-05     evaluation reward: 1.9655172413793103\n",
      "episode: 29   score: 1.0   memory length: 6046   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.9333333333333333\n",
      "episode: 30   score: 2.0   memory length: 6264   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.935483870967742\n",
      "episode: 31   score: 1.0   memory length: 6416   epsilon: 1.0    steps: 152    lr: 1e-05     evaluation reward: 1.90625\n",
      "episode: 32   score: 2.0   memory length: 6614   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.9090909090909092\n",
      "episode: 33   score: 0.0   memory length: 6736   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.8529411764705883\n",
      "episode: 34   score: 0.0   memory length: 6859   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.8\n",
      "episode: 35   score: 0.0   memory length: 6981   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.75\n",
      "episode: 36   score: 1.0   memory length: 7150   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.7297297297297298\n",
      "episode: 37   score: 0.0   memory length: 7273   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.6842105263157894\n",
      "episode: 38   score: 4.0   memory length: 7587   epsilon: 1.0    steps: 314    lr: 1e-05     evaluation reward: 1.7435897435897436\n",
      "episode: 39   score: 2.0   memory length: 7804   epsilon: 1.0    steps: 217    lr: 1e-05     evaluation reward: 1.75\n",
      "episode: 40   score: 1.0   memory length: 7955   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.7317073170731707\n",
      "episode: 41   score: 2.0   memory length: 8153   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.7380952380952381\n",
      "episode: 42   score: 1.0   memory length: 8323   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.7209302325581395\n",
      "episode: 43   score: 3.0   memory length: 8569   epsilon: 1.0    steps: 246    lr: 1e-05     evaluation reward: 1.75\n",
      "episode: 44   score: 0.0   memory length: 8692   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.711111111111111\n",
      "episode: 45   score: 1.0   memory length: 8864   epsilon: 1.0    steps: 172    lr: 1e-05     evaluation reward: 1.6956521739130435\n",
      "episode: 46   score: 0.0   memory length: 8987   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.6595744680851063\n",
      "episode: 47   score: 0.0   memory length: 9110   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.625\n",
      "episode: 48   score: 0.0   memory length: 9232   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.5918367346938775\n",
      "episode: 49   score: 1.0   memory length: 9400   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.58\n",
      "episode: 50   score: 2.0   memory length: 9600   epsilon: 1.0    steps: 200    lr: 1e-05     evaluation reward: 1.588235294117647\n",
      "episode: 51   score: 2.0   memory length: 9820   epsilon: 1.0    steps: 220    lr: 1e-05     evaluation reward: 1.5961538461538463\n",
      "episode: 52   score: 4.0   memory length: 10116   epsilon: 1.0    steps: 296    lr: 1e-05     evaluation reward: 1.6415094339622642\n",
      "episode: 53   score: 2.0   memory length: 10334   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.6481481481481481\n",
      "episode: 54   score: 3.0   memory length: 10579   epsilon: 1.0    steps: 245    lr: 1e-05     evaluation reward: 1.6727272727272726\n",
      "episode: 55   score: 2.0   memory length: 10776   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.6785714285714286\n",
      "episode: 56   score: 1.0   memory length: 10927   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.6666666666666667\n",
      "episode: 57   score: 0.0   memory length: 11049   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.6379310344827587\n",
      "episode: 58   score: 1.0   memory length: 11217   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.6271186440677967\n",
      "episode: 59   score: 1.0   memory length: 11387   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.6166666666666667\n",
      "episode: 60   score: 0.0   memory length: 11510   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.5901639344262295\n",
      "episode: 61   score: 0.0   memory length: 11633   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.564516129032258\n",
      "episode: 62   score: 0.0   memory length: 11755   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.5396825396825398\n",
      "episode: 63   score: 2.0   memory length: 11953   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.546875\n",
      "episode: 64   score: 3.0   memory length: 12180   epsilon: 1.0    steps: 227    lr: 1e-05     evaluation reward: 1.5692307692307692\n",
      "episode: 65   score: 2.0   memory length: 12381   epsilon: 1.0    steps: 201    lr: 1e-05     evaluation reward: 1.5757575757575757\n",
      "episode: 66   score: 2.0   memory length: 12578   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.5820895522388059\n",
      "episode: 67   score: 2.0   memory length: 12776   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.588235294117647\n",
      "episode: 68   score: 1.0   memory length: 12927   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.5797101449275361\n",
      "episode: 69   score: 1.0   memory length: 13096   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.5714285714285714\n",
      "episode: 70   score: 1.0   memory length: 13265   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.5633802816901408\n",
      "episode: 71   score: 1.0   memory length: 13434   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.5555555555555556\n",
      "episode: 72   score: 0.0   memory length: 13556   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.5342465753424657\n",
      "episode: 73   score: 2.0   memory length: 13754   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.5405405405405406\n",
      "episode: 74   score: 1.0   memory length: 13926   epsilon: 1.0    steps: 172    lr: 1e-05     evaluation reward: 1.5333333333333334\n",
      "episode: 75   score: 0.0   memory length: 14049   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.513157894736842\n",
      "episode: 76   score: 0.0   memory length: 14173   epsilon: 1.0    steps: 124    lr: 1e-05     evaluation reward: 1.4935064935064934\n",
      "episode: 77   score: 2.0   memory length: 14370   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.5\n",
      "episode: 78   score: 2.0   memory length: 14568   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.5063291139240507\n",
      "episode: 79   score: 2.0   memory length: 14766   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.5125\n",
      "episode: 80   score: 1.0   memory length: 14934   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.5061728395061729\n",
      "episode: 81   score: 0.0   memory length: 15057   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4878048780487805\n",
      "episode: 82   score: 2.0   memory length: 15275   epsilon: 1.0    steps: 218    lr: 1e-05     evaluation reward: 1.4939759036144578\n",
      "episode: 83   score: 3.0   memory length: 15524   epsilon: 1.0    steps: 249    lr: 1e-05     evaluation reward: 1.5119047619047619\n",
      "episode: 84   score: 1.0   memory length: 15693   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.5058823529411764\n",
      "episode: 85   score: 2.0   memory length: 15891   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.5116279069767442\n",
      "episode: 86   score: 0.0   memory length: 16014   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4942528735632183\n",
      "episode: 87   score: 1.0   memory length: 16183   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.4886363636363635\n",
      "episode: 88   score: 0.0   memory length: 16306   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4719101123595506\n",
      "episode: 89   score: 0.0   memory length: 16428   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.4555555555555555\n",
      "episode: 90   score: 1.0   memory length: 16597   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.4505494505494505\n",
      "episode: 91   score: 4.0   memory length: 16913   epsilon: 1.0    steps: 316    lr: 1e-05     evaluation reward: 1.4782608695652173\n",
      "episode: 92   score: 2.0   memory length: 17111   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.4838709677419355\n",
      "episode: 93   score: 2.0   memory length: 17309   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.4893617021276595\n",
      "episode: 94   score: 3.0   memory length: 17554   epsilon: 1.0    steps: 245    lr: 1e-05     evaluation reward: 1.5052631578947369\n",
      "episode: 95   score: 0.0   memory length: 17677   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.4895833333333333\n",
      "episode: 96   score: 3.0   memory length: 17903   epsilon: 1.0    steps: 226    lr: 1e-05     evaluation reward: 1.5051546391752577\n",
      "episode: 97   score: 2.0   memory length: 18101   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.510204081632653\n",
      "episode: 98   score: 0.0   memory length: 18224   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.494949494949495\n",
      "episode: 99   score: 0.0   memory length: 18347   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.48\n",
      "episode: 100   score: 2.0   memory length: 18544   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.48\n",
      "episode: 101   score: 1.0   memory length: 18694   epsilon: 1.0    steps: 150    lr: 1e-05     evaluation reward: 1.49\n",
      "episode: 102   score: 4.0   memory length: 18971   epsilon: 1.0    steps: 277    lr: 1e-05     evaluation reward: 1.51\n",
      "episode: 103   score: 3.0   memory length: 19202   epsilon: 1.0    steps: 231    lr: 1e-05     evaluation reward: 1.53\n",
      "episode: 104   score: 3.0   memory length: 19433   epsilon: 1.0    steps: 231    lr: 1e-05     evaluation reward: 1.54\n",
      "episode: 105   score: 0.0   memory length: 19555   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.54\n",
      "episode: 106   score: 2.0   memory length: 19753   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.54\n",
      "episode: 107   score: 1.0   memory length: 19924   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.5\n",
      "episode: 108   score: 0.0   memory length: 20046   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.47\n",
      "episode: 109   score: 1.0   memory length: 20217   epsilon: 1.0    steps: 171    lr: 1e-05     evaluation reward: 1.48\n",
      "episode: 110   score: 4.0   memory length: 20530   epsilon: 1.0    steps: 313    lr: 1e-05     evaluation reward: 1.51\n",
      "episode: 111   score: 2.0   memory length: 20747   epsilon: 1.0    steps: 217    lr: 1e-05     evaluation reward: 1.53\n",
      "episode: 112   score: 1.0   memory length: 20915   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.54\n",
      "episode: 113   score: 2.0   memory length: 21113   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.52\n",
      "episode: 114   score: 1.0   memory length: 21264   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.53\n",
      "episode: 115   score: 1.0   memory length: 21415   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.5\n",
      "episode: 116   score: 1.0   memory length: 21585   epsilon: 1.0    steps: 170    lr: 1e-05     evaluation reward: 1.47\n",
      "episode: 117   score: 3.0   memory length: 21833   epsilon: 1.0    steps: 248    lr: 1e-05     evaluation reward: 1.48\n",
      "episode: 118   score: 0.0   memory length: 21956   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.44\n",
      "episode: 119   score: 3.0   memory length: 22203   epsilon: 1.0    steps: 247    lr: 1e-05     evaluation reward: 1.44\n",
      "episode: 120   score: 3.0   memory length: 22448   epsilon: 1.0    steps: 245    lr: 1e-05     evaluation reward: 1.44\n",
      "episode: 121   score: 2.0   memory length: 22646   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.44\n",
      "episode: 122   score: 1.0   memory length: 22815   epsilon: 1.0    steps: 169    lr: 1e-05     evaluation reward: 1.45\n",
      "episode: 123   score: 3.0   memory length: 23063   epsilon: 1.0    steps: 248    lr: 1e-05     evaluation reward: 1.46\n",
      "episode: 124   score: 2.0   memory length: 23261   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.47\n",
      "episode: 125   score: 0.0   memory length: 23384   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.42\n",
      "episode: 126   score: 3.0   memory length: 23628   epsilon: 1.0    steps: 244    lr: 1e-05     evaluation reward: 1.43\n",
      "episode: 127   score: 1.0   memory length: 23779   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.44\n",
      "episode: 128   score: 4.0   memory length: 24056   epsilon: 1.0    steps: 277    lr: 1e-05     evaluation reward: 1.45\n",
      "episode: 129   score: 1.0   memory length: 24224   epsilon: 1.0    steps: 168    lr: 1e-05     evaluation reward: 1.45\n",
      "episode: 130   score: 0.0   memory length: 24347   epsilon: 1.0    steps: 123    lr: 1e-05     evaluation reward: 1.43\n",
      "episode: 131   score: 3.0   memory length: 24592   epsilon: 1.0    steps: 245    lr: 1e-05     evaluation reward: 1.45\n",
      "episode: 132   score: 2.0   memory length: 24790   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.45\n",
      "episode: 133   score: 2.0   memory length: 24988   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.47\n",
      "episode: 134   score: 2.0   memory length: 25186   epsilon: 1.0    steps: 198    lr: 1e-05     evaluation reward: 1.49\n",
      "episode: 135   score: 5.0   memory length: 25531   epsilon: 1.0    steps: 345    lr: 1e-05     evaluation reward: 1.54\n",
      "episode: 136   score: 1.0   memory length: 25682   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.54\n",
      "episode: 137   score: 3.0   memory length: 25929   epsilon: 1.0    steps: 247    lr: 1e-05     evaluation reward: 1.57\n",
      "episode: 138   score: 3.0   memory length: 26177   epsilon: 1.0    steps: 248    lr: 1e-05     evaluation reward: 1.56\n",
      "episode: 139   score: 1.0   memory length: 26328   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.55\n",
      "episode: 140   score: 1.0   memory length: 26479   epsilon: 1.0    steps: 151    lr: 1e-05     evaluation reward: 1.55\n",
      "episode: 141   score: 2.0   memory length: 26676   epsilon: 1.0    steps: 197    lr: 1e-05     evaluation reward: 1.55\n",
      "episode: 142   score: 6.0   memory length: 27059   epsilon: 1.0    steps: 383    lr: 1e-05     evaluation reward: 1.6\n",
      "episode: 143   score: 0.0   memory length: 27181   epsilon: 1.0    steps: 122    lr: 1e-05     evaluation reward: 1.57\n",
      "episode: 144   score: 2.0   memory length: 27396   epsilon: 1.0    steps: 215    lr: 1e-05     evaluation reward: 1.59\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     26\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/scratch/aalkhami/cs747/hw5/CS747_Assigment5/agent.py:46\u001b[0m, in \u001b[0;36mAgent.get_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = np.clip(reward, -1, 1) \n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 8 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) >= 8 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn_v2.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest_v2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "#from gym.wrappers.monitor import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v5')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['lives'])\n",
    "        \n",
    "    life = info['lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
