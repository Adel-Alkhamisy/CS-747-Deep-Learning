{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from rnn.AWD_LSTM import AWDLSTM\n",
    "import urllib.request\n",
    "import os\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate\n",
    "%matplotlib inline\n",
    "import pdb\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2+cu111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,:;'\\\"-+!?()/\\\\[]{}<>\"\n",
    "# vocab_mapping = {char: i for i, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len:  11842726\n",
      "test len:  1315859\n",
      "All characters:  0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download text file if it does not exist\n",
    "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "filename = \"language_data/input.txt\"\n",
    "if not os.path.isfile(filename):\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# Load and preprocess text\n",
    "file_path = 'language_data/input.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "all_characters = string.printable\n",
    "file_chars = [c for c in file if c in all_characters]\n",
    "char_to_idx = {char: i for i, char in enumerate(all_characters)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(all_characters)}\n",
    "vocab_size = len(all_characters)\n",
    "\n",
    "# Split data into train and test sets\n",
    "split = int(0.9 * len(file_chars))\n",
    "train_text = file_chars[:split]\n",
    "test_text = file_chars[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))\n",
    "print('All characters: ', all_characters)\n",
    "\n",
    "chunk_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " = Robert <unk> = \n",
      " \n",
      " Robert <unk> is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John <unk> in 2002 . In 2004 <unk> landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the <unk> <unk> Factory in London . He was directed by John <unk> and starred alongside Ben <unk> , Shane <unk> , Harry Kent , Fraser <unk> , Sophie Stanton and Dominic Hall . \n",
      " In 2006 , <unk> starred alongside <unk> in the play <unk> written by Mark <unk> . He appeared on a 2006 episode of the televisio\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download the WikiText-2 dataset\n",
    "url = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\"\n",
    "urllib.request.urlretrieve(url, \"wikitext-2-v1.zip\")\n",
    "\n",
    "# Extract the dataset\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"wikitext-2-v1.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "# Read in the text from the dataset\n",
    "with open(\"wikitext-2/wiki.test.tokens\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Get the first 1000 characters\n",
    "text_1000 = text[:1000]\n",
    "\n",
    "print(text_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'f', 'f', ' ', '.', ' ', 'I', 't', ' ', 'c', 'e', 'a', 's', 'e', 's', ' ', 'a', 'l', 'm', 'o', 's', 't', ' ', 'e', 'n', 't', 'i', 'r', 'e', 'l', 'y', ' ', 'n', 'e', 'a', 'r', ' ', 't', 'h', 'e', ' ', 'n', 'o', 'r', 't', 'h', 'e', 'r', 'n', ' ', 'c', 'i', 't', 'y', ' ', 'l', 'i', 'n', 'e', ' ', 'a', 't', ' ', 'Y', 'o', 'r', 'k', ' ', 'S', 't', 'r', 'e', 'e', 't', ' ', ',', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 't', 'h', 'e', ' ', 'h', 'o', 'm', 'e', 's', ' ', 'a', 'l', 'o', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'h', 'i', 'g', 'h', 'w', 'a', 'y', ' ', 'b', 'e', 'c', 'o', 'm', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 's', 'p', 'o', 'r', 'a', 'd', 'i', 'c', ' ', 'a', 'n', 'd', ' ', 's', 'p', 'a', 'c', 'e', 'd', ' ', 'a', 'p', 'a', 'r', 't', ' ', '.', ' ', '\\n', ' ', '\\n', ' ', '=', ' ', '=', ' ', '=', ' ', '=', ' ', 'N', 'o', 'r', 't', 'h', ' ', 'o', 'f', ' ', 'A', 'u', 'b', 'u', 'r', 'n', ' ', '=', ' ', '=', ' ', '=', ' ', '=', ' ', '\\n', ' ', '\\n', ' ', 'N', 'o', 'w', ' ', 'i', 'n', ' ', 't', 'h', 'e']\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tensor(string, batch_size=1):\n",
    "    if isinstance(string, str):\n",
    "        string = [string]\n",
    "    tensor = torch.zeros(len(string), batch_size).long()\n",
    "    for b in range(batch_size):\n",
    "        for c in range(len(string)):\n",
    "            char = string[c][b]\n",
    "            if char in char_to_idx:\n",
    "                tensor[c][b] = char_to_idx[char]\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def load_random_batch(text, seq_length, batch_size):\n",
    "    inputs = torch.zeros(seq_length, batch_size).long()\n",
    "    targets = torch.zeros(seq_length, batch_size).long()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - seq_length)\n",
    "        end_index = start_index + seq_length + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        inputs[:, i] = char_tensor(chunk[:-1], batch_size=1)[:, 0]\n",
    "        targets[:, i] = char_tensor(chunk[1:], batch_size=1)[:, 0]\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "def text_to_tensor(text, vocab_mapping):\n",
    "    indices = [vocab_mapping[char] for char in text]\n",
    "    return torch.tensor(indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prime_str='A', predict_len=100, temperature=0.8, device='cpu'):\n",
    "    hidden = (torch.zeros(model.num_layers, 2, model.hidden_size, device=device),\n",
    "              torch.zeros(model.num_layers, 2, model.hidden_size, device=device))\n",
    "    prime_input = torch.tensor([char_to_idx[c] for c in prime_str], dtype=torch.long, device=device)\n",
    "    generated_text = prime_str\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(predict_len):\n",
    "            output, hidden = model(prime_input.view(1, -1), hidden)\n",
    "            output_dist = output.squeeze().div(temperature).exp()\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]\n",
    "            generated_char = idx_to_char[top_i]\n",
    "            generated_text += generated_char\n",
    "            prime_input.fill_(top_i)\n",
    "\n",
    "    return generated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(inp.shape[1], device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(inp.shape[1], -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = vocab_size\n",
    "embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 10\n",
    "dropout = 0.05\n",
    "weight_dropout = 0.05\n",
    "batch_size = 1024\n",
    "seq_length = 1024\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5000\n",
    "bptt=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len:  8633346\n",
      "valid len:  4525239\n",
      "Epoch [10/5000], Loss: 5.5386\n",
      "Epoch [20/5000], Loss: 5.4542\n",
      "Epoch [30/5000], Loss: 5.4490\n",
      "Epoch [40/5000], Loss: 5.4462\n",
      "Epoch [50/5000], Loss: 5.4345\n",
      "Epoch [60/5000], Loss: 5.4241\n",
      "Epoch [70/5000], Loss: 5.4196\n",
      "Epoch [80/5000], Loss: 5.4138\n",
      "Epoch [90/5000], Loss: 5.4149\n",
      "Epoch [100/5000], Loss: 5.4143\n",
      "Epoch [110/5000], Loss: 5.4095\n",
      "Epoch [120/5000], Loss: 5.4006\n",
      "Epoch [130/5000], Loss: 5.4113\n",
      "Epoch [140/5000], Loss: 5.3985\n",
      "Epoch [150/5000], Loss: 5.3948\n",
      "Epoch [160/5000], Loss: 5.3997\n",
      "Epoch [170/5000], Loss: 5.4008\n",
      "Epoch [180/5000], Loss: 5.3906\n",
      "Epoch [190/5000], Loss: 5.3897\n",
      "Epoch [200/5000], Loss: 5.3887\n",
      "Epoch [210/5000], Loss: 5.4044\n",
      "Epoch [220/5000], Loss: 5.3958\n",
      "Epoch [230/5000], Loss: 5.3968\n",
      "Epoch [240/5000], Loss: 5.3842\n",
      "Epoch [250/5000], Loss: 5.3705\n",
      "Epoch [260/5000], Loss: 5.3775\n",
      "Epoch [270/5000], Loss: 5.3843\n",
      "Epoch [280/5000], Loss: 5.3791\n",
      "Epoch [290/5000], Loss: 5.3737\n",
      "Epoch [300/5000], Loss: 5.3857\n",
      "Epoch [310/5000], Loss: 5.3714\n",
      "Epoch [320/5000], Loss: 5.3767\n",
      "Epoch [330/5000], Loss: 5.3678\n",
      "Epoch [340/5000], Loss: 5.3724\n",
      "Epoch [350/5000], Loss: 5.3624\n",
      "Epoch [360/5000], Loss: 5.3522\n",
      "Epoch [370/5000], Loss: 5.3581\n",
      "Epoch [380/5000], Loss: 5.3445\n",
      "Epoch [390/5000], Loss: 5.3539\n",
      "Epoch [400/5000], Loss: 5.3556\n",
      "Epoch [410/5000], Loss: 5.3553\n",
      "Epoch [420/5000], Loss: 5.3551\n",
      "Epoch [430/5000], Loss: 5.3427\n",
      "Epoch [440/5000], Loss: 5.3421\n",
      "Epoch [450/5000], Loss: 5.3478\n",
      "Epoch [460/5000], Loss: 5.3349\n",
      "Epoch [470/5000], Loss: 5.3340\n",
      "Epoch [480/5000], Loss: 5.3155\n",
      "Epoch [490/5000], Loss: 5.3147\n",
      "Epoch [500/5000], Loss: 5.3047\n",
      "Epoch [510/5000], Loss: 5.3131\n",
      "Epoch [520/5000], Loss: 5.2845\n",
      "Epoch [530/5000], Loss: 5.2834\n",
      "Epoch [540/5000], Loss: 5.2811\n",
      "Epoch [550/5000], Loss: 5.2614\n",
      "Epoch [560/5000], Loss: 5.2585\n",
      "Epoch [570/5000], Loss: 5.2529\n",
      "Epoch [580/5000], Loss: 5.2146\n",
      "Epoch [590/5000], Loss: 5.2156\n",
      "Epoch [600/5000], Loss: 5.1840\n",
      "Epoch [610/5000], Loss: 5.1728\n",
      "Epoch [620/5000], Loss: 5.1492\n",
      "Epoch [630/5000], Loss: 5.1135\n",
      "Epoch [640/5000], Loss: 5.1023\n",
      "Epoch [650/5000], Loss: 5.0658\n",
      "Epoch [660/5000], Loss: 5.0638\n",
      "Epoch [670/5000], Loss: 5.0476\n",
      "Epoch [680/5000], Loss: 5.0553\n",
      "Epoch [690/5000], Loss: 5.0239\n",
      "Epoch [700/5000], Loss: 5.0156\n",
      "Epoch [710/5000], Loss: 5.0134\n",
      "Epoch [720/5000], Loss: 5.0114\n",
      "Epoch [730/5000], Loss: 5.0196\n",
      "Epoch [740/5000], Loss: 5.0157\n",
      "Epoch [750/5000], Loss: 5.0101\n",
      "Epoch [760/5000], Loss: 4.9983\n",
      "Epoch [770/5000], Loss: 5.0122\n",
      "Epoch [780/5000], Loss: 5.0107\n",
      "Epoch [790/5000], Loss: 5.0165\n",
      "Epoch [800/5000], Loss: 5.0058\n",
      "Epoch [810/5000], Loss: 4.9988\n",
      "Epoch [820/5000], Loss: 5.0118\n",
      "Epoch [830/5000], Loss: 5.0001\n",
      "Epoch [840/5000], Loss: 5.0046\n",
      "Epoch [850/5000], Loss: 5.0070\n",
      "Epoch [860/5000], Loss: 5.0077\n",
      "Epoch   866: reducing learning rate of group 0 to 9.9000e-04.\n",
      "Epoch [870/5000], Loss: 5.0073\n",
      "Epoch [880/5000], Loss: 5.0079\n",
      "Epoch [890/5000], Loss: 4.9916\n",
      "Epoch [900/5000], Loss: 5.0090\n",
      "Epoch [910/5000], Loss: 5.0041\n",
      "Epoch [920/5000], Loss: 4.9947\n",
      "Epoch [930/5000], Loss: 5.0035\n",
      "Epoch [940/5000], Loss: 4.9988\n",
      "Epoch [950/5000], Loss: 4.9990\n",
      "Epoch [960/5000], Loss: 4.9840\n",
      "Epoch [970/5000], Loss: 4.9985\n",
      "Epoch [980/5000], Loss: 4.9886\n",
      "Epoch [990/5000], Loss: 4.9941\n",
      "Epoch [1000/5000], Loss: 4.9860\n",
      "Epoch [1010/5000], Loss: 4.9995\n",
      "Epoch [1020/5000], Loss: 4.9954\n",
      "Epoch [1030/5000], Loss: 4.9943\n",
      "Epoch [1040/5000], Loss: 5.0071\n",
      "Epoch [1050/5000], Loss: 4.9933\n",
      "Epoch [1060/5000], Loss: 4.9998\n",
      "Epoch [1070/5000], Loss: 4.9935\n",
      "Epoch [1080/5000], Loss: 5.0123\n",
      "Epoch [1090/5000], Loss: 5.0076\n",
      "Epoch [1100/5000], Loss: 5.0024\n",
      "Epoch [1110/5000], Loss: 5.0025\n",
      "Epoch [1120/5000], Loss: 5.0081\n",
      "Epoch [1130/5000], Loss: 4.9939\n",
      "Epoch [1140/5000], Loss: 4.9959\n",
      "Epoch [1150/5000], Loss: 4.9853\n",
      "Epoch [1160/5000], Loss: 5.0077\n",
      "Epoch [1170/5000], Loss: 5.0006\n",
      "Epoch [1180/5000], Loss: 5.0014\n",
      "Epoch  1186: reducing learning rate of group 0 to 9.8010e-04.\n",
      "Epoch [1190/5000], Loss: 5.0027\n",
      "Epoch [1200/5000], Loss: 4.9871\n",
      "Epoch [1210/5000], Loss: 4.9899\n",
      "Epoch [1220/5000], Loss: 5.0019\n",
      "Epoch [1230/5000], Loss: 5.0037\n",
      "Epoch [1240/5000], Loss: 5.0060\n",
      "Epoch [1250/5000], Loss: 5.0018\n",
      "Epoch [1260/5000], Loss: 5.0060\n",
      "Epoch [1270/5000], Loss: 4.9938\n",
      "Epoch [1280/5000], Loss: 4.9991\n",
      "Epoch  1287: reducing learning rate of group 0 to 9.7030e-04.\n",
      "Epoch [1290/5000], Loss: 4.9951\n",
      "Epoch [1300/5000], Loss: 5.0087\n",
      "Epoch [1310/5000], Loss: 4.9853\n",
      "Epoch [1320/5000], Loss: 5.0192\n",
      "Epoch [1330/5000], Loss: 4.9867\n",
      "Epoch [1340/5000], Loss: 4.9964\n",
      "Epoch [1350/5000], Loss: 5.0034\n",
      "Epoch [1360/5000], Loss: 4.9868\n",
      "Epoch [1370/5000], Loss: 5.0017\n",
      "Epoch [1380/5000], Loss: 4.9893\n",
      "Epoch  1388: reducing learning rate of group 0 to 9.6060e-04.\n",
      "Epoch [1390/5000], Loss: 5.0093\n",
      "Epoch [1400/5000], Loss: 4.9940\n",
      "Epoch [1410/5000], Loss: 5.0091\n",
      "Epoch [1420/5000], Loss: 4.9989\n",
      "Epoch [1430/5000], Loss: 4.9921\n",
      "Epoch [1440/5000], Loss: 4.9896\n",
      "Epoch [1450/5000], Loss: 4.9927\n",
      "Epoch [1460/5000], Loss: 4.9986\n",
      "Epoch [1470/5000], Loss: 5.0131\n",
      "Epoch [1480/5000], Loss: 4.9922\n",
      "Epoch  1489: reducing learning rate of group 0 to 9.5099e-04.\n",
      "Epoch [1490/5000], Loss: 5.0055\n",
      "Epoch [1500/5000], Loss: 4.9916\n",
      "Epoch [1510/5000], Loss: 4.9981\n",
      "Epoch [1520/5000], Loss: 4.9914\n",
      "Epoch [1530/5000], Loss: 5.0009\n",
      "Epoch [1540/5000], Loss: 5.0097\n",
      "Epoch [1550/5000], Loss: 5.0005\n",
      "Epoch [1560/5000], Loss: 4.9914\n",
      "Epoch [1570/5000], Loss: 5.0004\n",
      "Epoch [1580/5000], Loss: 5.0025\n",
      "Epoch [1590/5000], Loss: 4.9916\n",
      "Epoch  1590: reducing learning rate of group 0 to 9.4148e-04.\n",
      "Epoch [1600/5000], Loss: 4.9911\n",
      "Epoch [1610/5000], Loss: 5.0050\n",
      "Epoch [1620/5000], Loss: 5.0035\n",
      "Epoch [1630/5000], Loss: 4.9893\n",
      "Epoch [1640/5000], Loss: 4.9923\n",
      "Epoch [1650/5000], Loss: 4.9925\n",
      "Epoch [1660/5000], Loss: 4.9885\n",
      "Epoch [1670/5000], Loss: 5.0056\n",
      "Epoch [1680/5000], Loss: 5.0060\n",
      "Epoch [1690/5000], Loss: 5.0075\n",
      "Epoch  1691: reducing learning rate of group 0 to 9.3207e-04.\n",
      "Epoch [1700/5000], Loss: 4.9922\n",
      "Epoch [1710/5000], Loss: 4.9970\n",
      "Epoch [1720/5000], Loss: 4.9888\n",
      "Epoch [1730/5000], Loss: 4.9922\n",
      "Epoch [1740/5000], Loss: 5.0007\n",
      "Epoch [1750/5000], Loss: 4.9995\n",
      "Epoch [1760/5000], Loss: 4.9886\n",
      "Epoch [1770/5000], Loss: 4.9884\n",
      "Epoch [1780/5000], Loss: 5.0019\n",
      "Epoch [1790/5000], Loss: 4.9986\n",
      "Epoch  1792: reducing learning rate of group 0 to 9.2274e-04.\n",
      "Epoch [1800/5000], Loss: 4.9976\n",
      "Epoch [1810/5000], Loss: 5.0002\n",
      "Epoch [1820/5000], Loss: 5.0015\n",
      "Epoch [1830/5000], Loss: 4.9957\n",
      "Epoch [1840/5000], Loss: 5.0029\n",
      "Epoch [1850/5000], Loss: 4.9949\n",
      "Epoch [1860/5000], Loss: 4.9977\n",
      "Epoch [1870/5000], Loss: 4.9894\n",
      "Epoch [1880/5000], Loss: 5.0014\n",
      "Epoch [1890/5000], Loss: 4.9993\n",
      "Epoch  1893: reducing learning rate of group 0 to 9.1352e-04.\n",
      "Epoch [1900/5000], Loss: 5.0091\n",
      "Epoch [1910/5000], Loss: 4.9907\n",
      "Epoch [1920/5000], Loss: 4.9939\n",
      "Epoch [1930/5000], Loss: 5.0088\n",
      "Epoch [1940/5000], Loss: 5.0026\n",
      "Epoch [1950/5000], Loss: 5.0091\n",
      "Epoch [1960/5000], Loss: 4.9980\n",
      "Epoch [1970/5000], Loss: 5.0030\n",
      "Epoch [1980/5000], Loss: 5.0019\n",
      "Epoch [1990/5000], Loss: 5.0010\n",
      "Epoch  1994: reducing learning rate of group 0 to 9.0438e-04.\n",
      "Epoch [2000/5000], Loss: 4.9976\n",
      "Epoch [2010/5000], Loss: 5.0052\n",
      "Epoch [2020/5000], Loss: 4.9901\n",
      "Epoch [2030/5000], Loss: 4.9937\n",
      "Epoch [2040/5000], Loss: 5.0045\n",
      "Epoch [2050/5000], Loss: 4.9867\n",
      "Epoch [2060/5000], Loss: 5.0063\n",
      "Epoch [2070/5000], Loss: 5.0035\n",
      "Epoch [2080/5000], Loss: 5.0092\n",
      "Epoch [2090/5000], Loss: 4.9987\n",
      "Epoch  2095: reducing learning rate of group 0 to 8.9534e-04.\n",
      "Epoch [2100/5000], Loss: 4.9959\n",
      "Epoch [2110/5000], Loss: 4.9865\n",
      "Epoch [2120/5000], Loss: 5.0070\n",
      "Epoch [2130/5000], Loss: 5.0032\n",
      "Epoch [2140/5000], Loss: 4.9880\n",
      "Epoch [2150/5000], Loss: 4.9886\n",
      "Epoch [2160/5000], Loss: 4.9913\n",
      "Epoch [2170/5000], Loss: 4.9985\n",
      "Epoch [2180/5000], Loss: 5.0047\n",
      "Epoch [2190/5000], Loss: 5.0015\n",
      "Epoch  2196: reducing learning rate of group 0 to 8.8638e-04.\n",
      "Epoch [2200/5000], Loss: 4.9976\n",
      "Epoch [2210/5000], Loss: 4.9994\n",
      "Epoch [2220/5000], Loss: 5.0072\n",
      "Epoch [2230/5000], Loss: 4.9985\n",
      "Epoch [2240/5000], Loss: 4.9934\n",
      "Epoch [2250/5000], Loss: 5.0025\n",
      "Epoch [2260/5000], Loss: 4.9929\n",
      "Epoch [2270/5000], Loss: 5.0035\n",
      "Epoch [2280/5000], Loss: 4.9917\n",
      "Epoch [2290/5000], Loss: 5.0019\n",
      "Epoch  2297: reducing learning rate of group 0 to 8.7752e-04.\n",
      "Epoch [2300/5000], Loss: 4.9815\n",
      "Epoch [2310/5000], Loss: 5.0026\n",
      "Epoch [2320/5000], Loss: 4.9974\n",
      "Epoch [2330/5000], Loss: 5.0090\n",
      "Epoch [2340/5000], Loss: 5.0008\n",
      "Epoch [2350/5000], Loss: 4.9986\n",
      "Epoch [2360/5000], Loss: 4.9907\n",
      "Epoch [2370/5000], Loss: 4.9981\n",
      "Epoch [2380/5000], Loss: 4.9964\n",
      "Epoch [2390/5000], Loss: 4.9911\n",
      "Epoch  2398: reducing learning rate of group 0 to 8.6875e-04.\n",
      "Epoch [2400/5000], Loss: 4.9966\n",
      "Epoch [2410/5000], Loss: 5.0117\n",
      "Epoch [2420/5000], Loss: 5.0017\n",
      "Epoch [2430/5000], Loss: 5.0103\n",
      "Epoch [2440/5000], Loss: 5.0018\n",
      "Epoch [2450/5000], Loss: 4.9969\n",
      "Epoch [2460/5000], Loss: 4.9906\n",
      "Epoch [2470/5000], Loss: 5.0147\n",
      "Epoch [2480/5000], Loss: 5.0057\n",
      "Epoch [2490/5000], Loss: 4.9985\n",
      "Epoch  2499: reducing learning rate of group 0 to 8.6006e-04.\n",
      "Epoch [2500/5000], Loss: 5.0015\n",
      "Epoch [2510/5000], Loss: 4.9977\n",
      "Epoch [2520/5000], Loss: 5.0034\n",
      "Epoch [2530/5000], Loss: 4.9991\n",
      "Epoch [2540/5000], Loss: 4.9878\n",
      "Epoch [2550/5000], Loss: 4.9916\n",
      "Epoch [2560/5000], Loss: 5.0077\n",
      "Epoch [2570/5000], Loss: 5.0038\n",
      "Epoch [2580/5000], Loss: 4.9854\n",
      "Epoch [2590/5000], Loss: 5.0131\n",
      "Epoch [2600/5000], Loss: 4.9924\n",
      "Epoch  2600: reducing learning rate of group 0 to 8.5146e-04.\n",
      "Epoch [2610/5000], Loss: 4.9963\n",
      "Epoch [2620/5000], Loss: 5.0036\n",
      "Epoch [2630/5000], Loss: 4.9980\n",
      "Epoch [2640/5000], Loss: 4.9900\n",
      "Epoch [2650/5000], Loss: 4.9983\n",
      "Epoch [2660/5000], Loss: 5.0009\n",
      "Epoch [2670/5000], Loss: 5.0005\n",
      "Epoch [2680/5000], Loss: 4.9954\n",
      "Epoch [2690/5000], Loss: 4.9993\n",
      "Epoch [2700/5000], Loss: 5.0004\n",
      "Epoch  2701: reducing learning rate of group 0 to 8.4294e-04.\n",
      "Epoch [2710/5000], Loss: 4.9909\n",
      "Epoch [2720/5000], Loss: 4.9939\n",
      "Epoch [2730/5000], Loss: 4.9967\n",
      "Epoch [2740/5000], Loss: 4.9990\n",
      "Epoch [2750/5000], Loss: 4.9912\n",
      "Epoch [2760/5000], Loss: 4.9837\n",
      "Epoch [2770/5000], Loss: 5.0002\n",
      "Epoch [2780/5000], Loss: 5.0081\n",
      "Epoch [2790/5000], Loss: 4.9935\n",
      "Epoch [2800/5000], Loss: 4.9959\n",
      "Epoch  2802: reducing learning rate of group 0 to 8.3451e-04.\n",
      "Epoch [2810/5000], Loss: 4.9959\n",
      "Epoch [2820/5000], Loss: 5.0043\n",
      "Epoch [2830/5000], Loss: 4.9898\n",
      "Epoch [2840/5000], Loss: 5.0031\n",
      "Epoch [2850/5000], Loss: 5.0096\n",
      "Epoch [2860/5000], Loss: 5.0018\n",
      "Epoch [2870/5000], Loss: 4.9943\n",
      "Epoch [2880/5000], Loss: 5.0011\n",
      "Epoch [2890/5000], Loss: 4.9884\n",
      "Epoch [2900/5000], Loss: 5.0006\n",
      "Epoch  2903: reducing learning rate of group 0 to 8.2617e-04.\n",
      "Epoch [2910/5000], Loss: 5.0052\n",
      "Epoch [2920/5000], Loss: 4.9881\n",
      "Epoch [2930/5000], Loss: 4.9925\n",
      "Epoch [2940/5000], Loss: 4.9894\n",
      "Epoch [2950/5000], Loss: 5.0001\n",
      "Epoch [2960/5000], Loss: 4.9807\n",
      "Epoch [2970/5000], Loss: 4.9928\n",
      "Epoch [2980/5000], Loss: 5.0019\n",
      "Epoch [2990/5000], Loss: 5.0008\n",
      "Epoch [3000/5000], Loss: 4.9909\n",
      "Epoch  3004: reducing learning rate of group 0 to 8.1791e-04.\n",
      "Epoch [3010/5000], Loss: 5.0005\n",
      "Epoch [3020/5000], Loss: 4.9884\n",
      "Epoch [3030/5000], Loss: 5.0093\n",
      "Epoch [3040/5000], Loss: 4.9930\n",
      "Epoch [3050/5000], Loss: 4.9955\n",
      "Epoch [3060/5000], Loss: 4.9888\n",
      "Epoch [3070/5000], Loss: 5.0046\n",
      "Epoch [3080/5000], Loss: 4.9934\n",
      "Epoch [3090/5000], Loss: 5.0158\n",
      "Epoch [3100/5000], Loss: 4.9956\n",
      "Epoch  3108: reducing learning rate of group 0 to 8.0973e-04.\n",
      "Epoch [3110/5000], Loss: 4.9935\n",
      "Epoch [3120/5000], Loss: 4.9958\n",
      "Epoch [3130/5000], Loss: 4.9878\n",
      "Epoch [3140/5000], Loss: 4.9896\n",
      "Epoch [3150/5000], Loss: 4.9833\n",
      "Epoch [3160/5000], Loss: 5.0158\n",
      "Epoch [3170/5000], Loss: 5.0017\n",
      "Epoch [3180/5000], Loss: 5.0046\n",
      "Epoch [3190/5000], Loss: 5.0106\n",
      "Epoch [3200/5000], Loss: 4.9884\n",
      "Epoch  3209: reducing learning rate of group 0 to 8.0163e-04.\n",
      "Epoch [3210/5000], Loss: 5.0008\n",
      "Epoch [3220/5000], Loss: 4.9950\n",
      "Epoch [3230/5000], Loss: 4.9950\n",
      "Epoch [3240/5000], Loss: 5.0017\n",
      "Epoch [3250/5000], Loss: 4.9949\n",
      "Epoch [3260/5000], Loss: 4.9948\n",
      "Epoch [3270/5000], Loss: 4.9926\n",
      "Epoch [3280/5000], Loss: 4.9927\n",
      "Epoch [3290/5000], Loss: 5.0053\n",
      "Epoch [3300/5000], Loss: 4.9964\n",
      "Epoch [3310/5000], Loss: 4.9963\n",
      "Epoch  3310: reducing learning rate of group 0 to 7.9361e-04.\n",
      "Epoch [3320/5000], Loss: 4.9961\n",
      "Epoch [3330/5000], Loss: 4.9892\n",
      "Epoch [3340/5000], Loss: 5.0012\n",
      "Epoch [3350/5000], Loss: 4.9914\n",
      "Epoch [3360/5000], Loss: 4.9954\n",
      "Epoch [3370/5000], Loss: 5.0138\n",
      "Epoch [3380/5000], Loss: 5.0022\n",
      "Epoch [3390/5000], Loss: 4.9919\n",
      "Epoch [3400/5000], Loss: 4.9986\n",
      "Epoch [3410/5000], Loss: 5.0161\n",
      "Epoch  3411: reducing learning rate of group 0 to 7.8568e-04.\n",
      "Epoch [3420/5000], Loss: 5.0080\n",
      "Epoch [3430/5000], Loss: 5.0007\n",
      "Epoch [3440/5000], Loss: 5.0032\n",
      "Epoch [3450/5000], Loss: 4.9969\n",
      "Epoch [3460/5000], Loss: 4.9968\n",
      "Epoch [3470/5000], Loss: 5.0014\n",
      "Epoch [3480/5000], Loss: 4.9876\n",
      "Epoch [3490/5000], Loss: 4.9916\n",
      "Epoch [3500/5000], Loss: 4.9964\n",
      "Epoch [3510/5000], Loss: 5.0181\n",
      "Epoch  3512: reducing learning rate of group 0 to 7.7782e-04.\n",
      "Epoch [3520/5000], Loss: 4.9925\n",
      "Epoch [3530/5000], Loss: 4.9947\n",
      "Epoch [3540/5000], Loss: 4.9926\n",
      "Epoch [3550/5000], Loss: 4.9932\n",
      "Epoch [3560/5000], Loss: 4.9935\n",
      "Epoch [3570/5000], Loss: 5.0038\n",
      "Epoch [3580/5000], Loss: 4.9936\n",
      "Epoch [3590/5000], Loss: 5.0036\n",
      "Epoch [3600/5000], Loss: 5.0047\n",
      "Epoch [3610/5000], Loss: 5.0013\n",
      "Epoch  3613: reducing learning rate of group 0 to 7.7004e-04.\n",
      "Epoch [3620/5000], Loss: 4.9983\n",
      "Epoch [3630/5000], Loss: 5.0064\n",
      "Epoch [3640/5000], Loss: 4.9863\n",
      "Epoch [3650/5000], Loss: 5.0001\n",
      "Epoch [3660/5000], Loss: 4.9958\n",
      "Epoch [3670/5000], Loss: 4.9831\n",
      "Epoch [3680/5000], Loss: 5.0028\n",
      "Epoch [3690/5000], Loss: 5.0039\n",
      "Epoch [3700/5000], Loss: 5.0080\n",
      "Epoch [3710/5000], Loss: 4.9949\n",
      "Epoch  3714: reducing learning rate of group 0 to 7.6234e-04.\n",
      "Epoch [3720/5000], Loss: 5.0101\n",
      "Epoch [3730/5000], Loss: 4.9957\n",
      "Epoch [3740/5000], Loss: 5.0024\n",
      "Epoch [3750/5000], Loss: 4.9945\n",
      "Epoch [3760/5000], Loss: 5.0005\n",
      "Epoch [3770/5000], Loss: 4.9976\n",
      "Epoch [3780/5000], Loss: 4.9876\n",
      "Epoch [3790/5000], Loss: 5.0008\n",
      "Epoch [3800/5000], Loss: 4.9955\n",
      "Epoch [3810/5000], Loss: 5.0071\n",
      "Epoch  3815: reducing learning rate of group 0 to 7.5472e-04.\n",
      "Epoch [3820/5000], Loss: 4.9953\n",
      "Epoch [3830/5000], Loss: 5.0085\n",
      "Epoch [3840/5000], Loss: 5.0029\n",
      "Epoch [3850/5000], Loss: 4.9926\n",
      "Epoch [3860/5000], Loss: 5.0061\n",
      "Epoch [3870/5000], Loss: 5.0001\n",
      "Epoch [3880/5000], Loss: 5.0056\n",
      "Epoch [3890/5000], Loss: 4.9887\n",
      "Epoch [3900/5000], Loss: 4.9886\n",
      "Epoch [3910/5000], Loss: 4.9999\n",
      "Epoch  3916: reducing learning rate of group 0 to 7.4717e-04.\n",
      "Epoch [3920/5000], Loss: 5.0007\n",
      "Epoch [3930/5000], Loss: 4.9957\n",
      "Epoch [3940/5000], Loss: 4.9970\n",
      "Epoch [3950/5000], Loss: 5.0055\n",
      "Epoch [3960/5000], Loss: 5.0089\n",
      "Epoch [3970/5000], Loss: 5.0000\n",
      "Epoch [3980/5000], Loss: 4.9960\n",
      "Epoch [3990/5000], Loss: 5.0072\n",
      "Epoch [4000/5000], Loss: 4.9998\n",
      "Epoch [4010/5000], Loss: 5.0018\n",
      "Epoch  4017: reducing learning rate of group 0 to 7.3970e-04.\n",
      "Epoch [4020/5000], Loss: 5.0182\n",
      "Epoch [4030/5000], Loss: 5.0021\n",
      "Epoch [4040/5000], Loss: 5.0009\n",
      "Epoch [4050/5000], Loss: 5.0001\n",
      "Epoch [4060/5000], Loss: 5.0030\n",
      "Epoch [4070/5000], Loss: 5.0048\n",
      "Epoch [4080/5000], Loss: 4.9975\n",
      "Epoch [4090/5000], Loss: 4.9895\n",
      "Epoch [4100/5000], Loss: 4.9894\n",
      "Epoch [4110/5000], Loss: 4.9847\n",
      "Epoch  4118: reducing learning rate of group 0 to 7.3230e-04.\n",
      "Epoch [4120/5000], Loss: 4.9980\n",
      "Epoch [4130/5000], Loss: 5.0024\n",
      "Epoch [4140/5000], Loss: 5.0138\n",
      "Epoch [4150/5000], Loss: 4.9974\n",
      "Epoch [4160/5000], Loss: 5.0162\n",
      "Epoch [4170/5000], Loss: 4.9947\n",
      "Epoch [4180/5000], Loss: 4.9965\n",
      "Epoch [4190/5000], Loss: 4.9970\n",
      "Epoch [4200/5000], Loss: 5.0018\n",
      "Epoch [4210/5000], Loss: 4.9948\n",
      "Epoch  4219: reducing learning rate of group 0 to 7.2498e-04.\n",
      "Epoch [4220/5000], Loss: 4.9922\n",
      "Epoch [4230/5000], Loss: 4.9792\n",
      "Epoch [4240/5000], Loss: 5.0003\n",
      "Epoch [4250/5000], Loss: 4.9997\n",
      "Epoch [4260/5000], Loss: 4.9887\n",
      "Epoch [4270/5000], Loss: 4.9937\n",
      "Epoch [4280/5000], Loss: 5.0025\n",
      "Epoch [4290/5000], Loss: 4.9966\n",
      "Epoch [4300/5000], Loss: 4.9962\n",
      "Epoch [4310/5000], Loss: 4.9951\n",
      "Epoch [4320/5000], Loss: 5.0100\n",
      "Epoch  4320: reducing learning rate of group 0 to 7.1773e-04.\n",
      "Epoch [4330/5000], Loss: 4.9956\n",
      "Epoch [4340/5000], Loss: 4.9985\n",
      "Epoch [4350/5000], Loss: 4.9971\n",
      "Epoch [4360/5000], Loss: 5.0052\n",
      "Epoch [4370/5000], Loss: 5.0011\n",
      "Epoch [4380/5000], Loss: 4.9904\n",
      "Epoch [4390/5000], Loss: 4.9883\n",
      "Epoch [4400/5000], Loss: 4.9902\n",
      "Epoch [4410/5000], Loss: 4.9957\n",
      "Epoch [4420/5000], Loss: 4.9926\n",
      "Epoch  4421: reducing learning rate of group 0 to 7.1055e-04.\n",
      "Epoch [4430/5000], Loss: 5.0083\n",
      "Epoch [4440/5000], Loss: 4.9916\n",
      "Epoch [4450/5000], Loss: 5.0028\n",
      "Epoch [4460/5000], Loss: 5.0095\n",
      "Epoch [4470/5000], Loss: 5.0038\n",
      "Epoch [4480/5000], Loss: 4.9938\n",
      "Epoch [4490/5000], Loss: 5.0099\n",
      "Epoch [4500/5000], Loss: 4.9941\n",
      "Epoch [4510/5000], Loss: 4.9976\n",
      "Epoch [4520/5000], Loss: 5.0065\n",
      "Epoch  4522: reducing learning rate of group 0 to 7.0345e-04.\n",
      "Epoch [4530/5000], Loss: 5.0030\n",
      "Epoch [4540/5000], Loss: 5.0059\n",
      "Epoch [4550/5000], Loss: 5.0035\n",
      "Epoch [4560/5000], Loss: 4.9924\n",
      "Epoch [4570/5000], Loss: 4.9969\n",
      "Epoch [4580/5000], Loss: 4.9985\n",
      "Epoch [4590/5000], Loss: 4.9969\n",
      "Epoch [4600/5000], Loss: 4.9995\n",
      "Epoch [4610/5000], Loss: 4.9896\n",
      "Epoch [4620/5000], Loss: 4.9839\n",
      "Epoch  4623: reducing learning rate of group 0 to 6.9641e-04.\n",
      "Epoch [4630/5000], Loss: 4.9962\n",
      "Epoch [4640/5000], Loss: 4.9939\n",
      "Epoch [4650/5000], Loss: 4.9988\n",
      "Epoch [4660/5000], Loss: 5.0086\n",
      "Epoch [4670/5000], Loss: 4.9972\n",
      "Epoch [4680/5000], Loss: 4.9961\n",
      "Epoch [4690/5000], Loss: 4.9908\n",
      "Epoch [4700/5000], Loss: 4.9882\n",
      "Epoch [4710/5000], Loss: 4.9914\n",
      "Epoch [4720/5000], Loss: 4.9970\n",
      "Epoch  4724: reducing learning rate of group 0 to 6.8945e-04.\n",
      "Epoch [4730/5000], Loss: 4.9898\n",
      "Epoch [4740/5000], Loss: 4.9904\n",
      "Epoch [4750/5000], Loss: 5.0075\n",
      "Epoch [4760/5000], Loss: 5.0088\n",
      "Epoch [4770/5000], Loss: 4.9999\n",
      "Epoch [4780/5000], Loss: 5.0030\n",
      "Epoch [4790/5000], Loss: 5.0043\n",
      "Epoch [4800/5000], Loss: 4.9932\n",
      "Epoch [4810/5000], Loss: 4.9823\n",
      "Epoch [4820/5000], Loss: 4.9914\n",
      "Epoch  4825: reducing learning rate of group 0 to 6.8255e-04.\n",
      "Epoch [4830/5000], Loss: 4.9988\n",
      "Epoch [4840/5000], Loss: 4.9931\n",
      "Epoch [4850/5000], Loss: 5.0037\n",
      "Epoch [4860/5000], Loss: 5.0061\n",
      "Epoch [4870/5000], Loss: 4.9816\n",
      "Epoch [4880/5000], Loss: 5.0009\n",
      "Epoch [4890/5000], Loss: 4.9940\n",
      "Epoch [4900/5000], Loss: 5.0010\n",
      "Epoch [4910/5000], Loss: 5.0050\n",
      "Epoch [4920/5000], Loss: 4.9895\n",
      "Epoch  4926: reducing learning rate of group 0 to 6.7573e-04.\n",
      "Epoch [4930/5000], Loss: 4.9970\n",
      "Epoch [4940/5000], Loss: 5.0065\n",
      "Epoch [4950/5000], Loss: 5.0006\n",
      "Epoch [4960/5000], Loss: 4.9909\n",
      "Epoch [4970/5000], Loss: 4.9998\n",
      "Epoch [4980/5000], Loss: 4.9991\n",
      "Epoch [4990/5000], Loss: 4.9989\n",
      "Epoch [5000/5000], Loss: 4.9915\n"
     ]
    }
   ],
   "source": [
    "# Define the model, loss function, optimizer, and learning rate scheduler\n",
    "input_size = len(all_characters)\n",
    "awd_lstm = AWDLSTM(input_size, hidden_size, num_layers, embedding_size=embedding_size, dropout=dropout, weight_dropout=weight_dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(awd_lstm.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.99, patience=100, verbose=True)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "split = int(0.9 * len(train_text))\n",
    "train_text = file_chars[:split]\n",
    "valid_text = file_chars[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('valid len: ', len(valid_text))\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_text, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_text, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "awd_lstm.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    hidden = None\n",
    "    num_batches = 0\n",
    "    for batch_idx in range(0, (len(file) - seq_length) // (batch_size * seq_length), batch_size):\n",
    "        input_batch, target_batch = load_random_batch(file[batch_idx*seq_length:(batch_idx+1)*seq_length], seq_length, batch_size)\n",
    "        num_batches += 1\n",
    "\n",
    "        for chunk_start in range(0, seq_length, bptt):\n",
    "            chunk_input = char_tensor(input_batch[:, chunk_start:chunk_start + bptt]).to(device)\n",
    "            chunk_target = char_tensor(target_batch[:, chunk_start:chunk_start + bptt]).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output, hidden = awd_lstm(chunk_input, hidden)\n",
    "            hidden = (hidden[0].detach(), hidden[1].detach())  # Detach hidden state to prevent backpropagation beyond the chunk\n",
    "            output = output.view(-1, output.size(2))\n",
    "            chunk_target = chunk_target.reshape(-1)\n",
    "            loss = criterion(output, chunk_target)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= num_batches\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Update the learning rate based on the epoch loss\n",
    "    scheduler.step(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(awd_lstm.state_dict(), './AWD_LSTM_generator_V3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to used the saved model\n",
    "saved_model_path = \"AWD_LSTM_generator_V3.pth\"\n",
    "awd_lstm = AWDLSTM(input_size, hidden_size, num_layers, embedding_size=embedding_size, dropout=dropout, weight_dropout=weight_dropout).to(device)\n",
    "awd_lstm.load_state_dict(torch.load(saved_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "           torch.zeros(num_layers, batch_size, hidden_size).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate(awd_lstm, prime_str='Th', predict_len=1000, vocab_mapping=char_to_idx, inv_vocab_mapping=idx_to_char, temperature=1.0)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNkFea7+VnxQpZiX82yZYG4",
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
