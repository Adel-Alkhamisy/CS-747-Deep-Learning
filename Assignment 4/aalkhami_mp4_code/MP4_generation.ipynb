{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6XQd_5xTJSd"
   },
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1680819320572,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "hcERtk6ZTJSf"
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2485,
     "status": "ok",
     "timestamp": 1680819326317,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "tyk38G82TJSh"
   },
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1680819327374,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "TAHRJmEjTJSh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_e-I3ieTJSh"
   },
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1841,
     "status": "ok",
     "timestamp": 1680819331454,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "Bt--rs3BTJSh",
    "outputId": "80368a7a-4ee3-43a0-a18f-eda810bf2d6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n",
      "train len:  1003854\n",
      "test len:  111540\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = 'language_data/shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1680819333879,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "899ZZ3ELTJSi",
    "outputId": "d9a67e21-64d6-47bd-caed-64df486ced7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1680819336588,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "nEW4PUObTJSi",
    "outputId": "53f07992-1e78-4cfd-87a0-554d2cef904b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "But, with the first of all your chief affairs,\n",
      "Let me entreat, for I command no more,\n",
      "That Margaret your queen and my son Edward\n",
      "Be sent for, to return from France with speed;\n",
      "For, till I see them he\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0LHlDBfTJSi"
   },
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FaYPPZPTJSi"
   },
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1680819349325,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "WXHCTmYZTJSj"
   },
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3pIddqlTJSj"
   },
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1680819352513,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "C-mI0yrVTJSj"
   },
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j39I5pHwTJSj"
   },
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQTvdcUFTJSj"
   },
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1680819356804,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "m2g8JXEsTJSk"
   },
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.0):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guIznBeJTJSk"
   },
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1680828066009,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "JCNG564sTJSk"
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "n_epochs = 5000\n",
    "hidden_size = 200\n",
    "n_layers = 1\n",
    "learning_rate = 0.01 #0.01\n",
    "model_type = 'lstm'\n",
    "print_every = 100\n",
    "plot_every = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1680828069386,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "n2ZHSP0rTJSk"
   },
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdgJBVY_TJSl"
   },
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1680828072011,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "YbruBn1JTJSl"
   },
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    \n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    hidden = rnn.init_hidden(input.size(0), device=device)\n",
    "    rnn.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for char_idx in range(input.size(1)):\n",
    "        output, hidden = rnn(input[:, char_idx], hidden)\n",
    "        loss += criterion(output, target[:, char_idx])\n",
    "\n",
    "    loss /= input.size(1)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=10)\n",
    "    optimizer.step()\n",
    "    ##########       END      ##########\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1952902,
     "status": "ok",
     "timestamp": 1680831347916,
     "user": {
      "displayName": "ADEL ALKHAMISY",
      "userId": "06010706832817633403"
     },
     "user_tz": 240
    },
    "id": "KgYbRr86TJSl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n",
      "Epoch 00086: reducing learning rate of group 0 to 9.9000e-03.\n",
      "[1m 3s (100 2%) train loss: 2.1080, test_loss: 2.1677]\n",
      "Whint. be and pay ain be to amer \n",
      "inge, we lost of kince auns, aind thear\" to me dik drothly, ford xat \n",
      "\n",
      "Epoch 00110: reducing learning rate of group 0 to 9.8010e-03.\n",
      "Epoch 00120: reducing learning rate of group 0 to 9.7030e-03.\n",
      "Epoch 00133: reducing learning rate of group 0 to 9.6060e-03.\n",
      "Epoch 00143: reducing learning rate of group 0 to 9.5099e-03.\n",
      "Epoch 00154: reducing learning rate of group 0 to 9.4148e-03.\n",
      "Epoch 00169: reducing learning rate of group 0 to 9.3207e-03.\n",
      "Epoch 00183: reducing learning rate of group 0 to 9.2274e-03.\n",
      "[2m 10s (200 4%) train loss: 1.9994, test_loss: 2.0707]\n",
      "Wher -oun with ares, as of rise wit a mall\n",
      "leer ming of of soor areA to one heply be the be, bity eibr \n",
      "\n",
      "Epoch 00220: reducing learning rate of group 0 to 9.1352e-03.\n",
      "Epoch 00226: reducing learning rate of group 0 to 9.0438e-03.\n",
      "Epoch 00232: reducing learning rate of group 0 to 8.9534e-03.\n",
      "Epoch 00239: reducing learning rate of group 0 to 8.8638e-03.\n",
      "Epoch 00250: reducing learning rate of group 0 to 8.7752e-03.\n",
      "Epoch 00256: reducing learning rate of group 0 to 8.6875e-03.\n",
      "Epoch 00262: reducing learning rate of group 0 to 8.6006e-03.\n",
      "Epoch 00270: reducing learning rate of group 0 to 8.5146e-03.\n",
      "Epoch 00276: reducing learning rate of group 0 to 8.4294e-03.\n",
      "Epoch 00285: reducing learning rate of group 0 to 8.3451e-03.\n",
      "Epoch 00291: reducing learning rate of group 0 to 8.2617e-03.\n",
      "[3m 15s (300 6%) train loss: 1.9174, test_loss: 2.0176]\n",
      "Whan mather with hsalI swo, dis, and the see.\n",
      "\n",
      "Sas, the and his bisted and he death you sitI you fortn \n",
      "\n",
      "Epoch 00303: reducing learning rate of group 0 to 8.1791e-03.\n",
      "Epoch 00309: reducing learning rate of group 0 to 8.0973e-03.\n",
      "Epoch 00315: reducing learning rate of group 0 to 8.0163e-03.\n",
      "Epoch 00321: reducing learning rate of group 0 to 7.9361e-03.\n",
      "Epoch 00339: reducing learning rate of group 0 to 7.8568e-03.\n",
      "Epoch 00345: reducing learning rate of group 0 to 7.7782e-03.\n",
      "Epoch 00351: reducing learning rate of group 0 to 7.7004e-03.\n",
      "Epoch 00357: reducing learning rate of group 0 to 7.6234e-03.\n",
      "Epoch 00369: reducing learning rate of group 0 to 7.5472e-03.\n",
      "Epoch 00375: reducing learning rate of group 0 to 7.4717e-03.\n",
      "Epoch 00381: reducing learning rate of group 0 to 7.3970e-03.\n",
      "Epoch 00389: reducing learning rate of group 0 to 7.3230e-03.\n",
      "Epoch 00395: reducing learning rate of group 0 to 7.2498e-03.\n",
      "[4m 18s (400 8%) train loss: 1.8873, test_loss: 1.9817]\n",
      "What will the bangelyros, iot not |alt, Castin\n",
      "the counet ou.\n",
      "What the loves he Sureny theuse:\n",
      "And foi \n",
      "\n",
      "Epoch 00401: reducing learning rate of group 0 to 7.1773e-03.\n",
      "Epoch 00407: reducing learning rate of group 0 to 7.1055e-03.\n",
      "Epoch 00424: reducing learning rate of group 0 to 7.0345e-03.\n",
      "Epoch 00430: reducing learning rate of group 0 to 6.9641e-03.\n",
      "Epoch 00441: reducing learning rate of group 0 to 6.8945e-03.\n",
      "Epoch 00447: reducing learning rate of group 0 to 6.8255e-03.\n",
      "Epoch 00456: reducing learning rate of group 0 to 6.7573e-03.\n",
      "Epoch 00462: reducing learning rate of group 0 to 6.6897e-03.\n",
      "Epoch 00468: reducing learning rate of group 0 to 6.6228e-03.\n",
      "Epoch 00474: reducing learning rate of group 0 to 6.5566e-03.\n",
      "Epoch 00480: reducing learning rate of group 0 to 6.4910e-03.\n",
      "Epoch 00489: reducing learning rate of group 0 to 6.4261e-03.\n",
      "Epoch 00495: reducing learning rate of group 0 to 6.3619e-03.\n",
      "[5m 24s (500 10%) train loss: 1.8711, test_loss: 1.9654]\n",
      "Whis true: and Healt\n",
      "Whnker; and all his bollforing al mC\n",
      "Forn thh to hard here low and tem, thels or  \n",
      "\n",
      "Epoch 00501: reducing learning rate of group 0 to 6.2982e-03.\n",
      "Epoch 00507: reducing learning rate of group 0 to 6.2353e-03.\n",
      "Epoch 00514: reducing learning rate of group 0 to 6.1729e-03.\n",
      "Epoch 00520: reducing learning rate of group 0 to 6.1112e-03.\n",
      "Epoch 00526: reducing learning rate of group 0 to 6.0501e-03.\n",
      "Epoch 00532: reducing learning rate of group 0 to 5.9896e-03.\n",
      "Epoch 00538: reducing learning rate of group 0 to 5.9297e-03.\n",
      "Epoch 00548: reducing learning rate of group 0 to 5.8704e-03.\n",
      "Epoch 00554: reducing learning rate of group 0 to 5.8117e-03.\n",
      "Epoch 00560: reducing learning rate of group 0 to 5.7535e-03.\n",
      "Epoch 00566: reducing learning rate of group 0 to 5.6960e-03.\n",
      "Epoch 00572: reducing learning rate of group 0 to 5.6391e-03.\n",
      "Epoch 00578: reducing learning rate of group 0 to 5.5827e-03.\n",
      "Epoch 00590: reducing learning rate of group 0 to 5.5268e-03.\n",
      "Epoch 00596: reducing learning rate of group 0 to 5.4716e-03.\n",
      "[6m 27s (600 12%) train loss: 1.8452, test_loss: 1.9412]\n",
      "Why at be Mnow,\n",
      "And it here air and will mar than  mees, and then thou face-will faret my woll in the  \n",
      "\n",
      "Epoch 00602: reducing learning rate of group 0 to 5.4169e-03.\n",
      "Epoch 00608: reducing learning rate of group 0 to 5.3627e-03.\n",
      "Epoch 00614: reducing learning rate of group 0 to 5.3091e-03.\n",
      "Epoch 00620: reducing learning rate of group 0 to 5.2560e-03.\n",
      "Epoch 00626: reducing learning rate of group 0 to 5.2034e-03.\n",
      "Epoch 00632: reducing learning rate of group 0 to 5.1514e-03.\n",
      "Epoch 00638: reducing learning rate of group 0 to 5.0999e-03.\n",
      "Epoch 00644: reducing learning rate of group 0 to 5.0489e-03.\n",
      "Epoch 00651: reducing learning rate of group 0 to 4.9984e-03.\n",
      "Epoch 00657: reducing learning rate of group 0 to 4.9484e-03.\n",
      "Epoch 00663: reducing learning rate of group 0 to 4.8989e-03.\n",
      "Epoch 00669: reducing learning rate of group 0 to 4.8499e-03.\n",
      "Epoch 00675: reducing learning rate of group 0 to 4.8014e-03.\n",
      "Epoch 00681: reducing learning rate of group 0 to 4.7534e-03.\n",
      "Epoch 00687: reducing learning rate of group 0 to 4.7059e-03.\n",
      "Epoch 00693: reducing learning rate of group 0 to 4.6588e-03.\n",
      "Epoch 00699: reducing learning rate of group 0 to 4.6122e-03.\n",
      "[7m 30s (700 14%) train loss: 1.8215, test_loss: 1.9293]\n",
      "Where who wer not stording and the alms wing.\n",
      "\n",
      "SICnnoolO, jower,\n",
      "And am biet guven a aa\n",
      "in our Callvi\n",
      " \n",
      "\n",
      "Epoch 00705: reducing learning rate of group 0 to 4.5661e-03.\n",
      "Epoch 00711: reducing learning rate of group 0 to 4.5204e-03.\n",
      "Epoch 00717: reducing learning rate of group 0 to 4.4752e-03.\n",
      "Epoch 00723: reducing learning rate of group 0 to 4.4305e-03.\n",
      "Epoch 00729: reducing learning rate of group 0 to 4.3862e-03.\n",
      "Epoch 00735: reducing learning rate of group 0 to 4.3423e-03.\n",
      "Epoch 00741: reducing learning rate of group 0 to 4.2989e-03.\n",
      "Epoch 00747: reducing learning rate of group 0 to 4.2559e-03.\n",
      "Epoch 00753: reducing learning rate of group 0 to 4.2133e-03.\n",
      "Epoch 00759: reducing learning rate of group 0 to 4.1712e-03.\n",
      "Epoch 00765: reducing learning rate of group 0 to 4.1295e-03.\n",
      "Epoch 00771: reducing learning rate of group 0 to 4.0882e-03.\n",
      "Epoch 00782: reducing learning rate of group 0 to 4.0473e-03.\n",
      "Epoch 00788: reducing learning rate of group 0 to 4.0068e-03.\n",
      "Epoch 00794: reducing learning rate of group 0 to 3.9668e-03.\n",
      "Epoch 00800: reducing learning rate of group 0 to 3.9271e-03.\n",
      "[8m 32s (800 16%) train loss: 1.8052, test_loss: 1.9354]\n",
      "WhoS cowens deather bine hear the one is it him,\n",
      "Une is grean ther'd as deaste's sulllory the heather  \n",
      "\n",
      "Epoch 00806: reducing learning rate of group 0 to 3.8878e-03.\n",
      "Epoch 00812: reducing learning rate of group 0 to 3.8490e-03.\n",
      "Epoch 00818: reducing learning rate of group 0 to 3.8105e-03.\n",
      "Epoch 00824: reducing learning rate of group 0 to 3.7724e-03.\n",
      "Epoch 00830: reducing learning rate of group 0 to 3.7346e-03.\n",
      "Epoch 00836: reducing learning rate of group 0 to 3.6973e-03.\n",
      "Epoch 00842: reducing learning rate of group 0 to 3.6603e-03.\n",
      "Epoch 00848: reducing learning rate of group 0 to 3.6237e-03.\n",
      "Epoch 00854: reducing learning rate of group 0 to 3.5875e-03.\n",
      "Epoch 00860: reducing learning rate of group 0 to 3.5516e-03.\n",
      "Epoch 00866: reducing learning rate of group 0 to 3.5161e-03.\n",
      "Epoch 00872: reducing learning rate of group 0 to 3.4809e-03.\n",
      "Epoch 00884: reducing learning rate of group 0 to 3.4461e-03.\n",
      "Epoch 00890: reducing learning rate of group 0 to 3.4117e-03.\n",
      "Epoch 00896: reducing learning rate of group 0 to 3.3775e-03.\n",
      "[9m 35s (900 18%) train loss: 1.8256, test_loss: 1.9318]\n",
      "Whe hornest Paride\n",
      "e whouth shear aT of in mprice: dains.\n",
      "\n",
      "et:\n",
      "Bule to the solk, aroud his beat olsern \n",
      "\n",
      "Epoch 00902: reducing learning rate of group 0 to 3.3438e-03.\n",
      "Epoch 00908: reducing learning rate of group 0 to 3.3103e-03.\n",
      "Epoch 00914: reducing learning rate of group 0 to 3.2772e-03.\n",
      "Epoch 00920: reducing learning rate of group 0 to 3.2445e-03.\n",
      "Epoch 00926: reducing learning rate of group 0 to 3.2120e-03.\n",
      "Epoch 00932: reducing learning rate of group 0 to 3.1799e-03.\n",
      "Epoch 00939: reducing learning rate of group 0 to 3.1481e-03.\n",
      "Epoch 00945: reducing learning rate of group 0 to 3.1166e-03.\n",
      "Epoch 00951: reducing learning rate of group 0 to 3.0854e-03.\n",
      "Epoch 00957: reducing learning rate of group 0 to 3.0546e-03.\n",
      "Epoch 00963: reducing learning rate of group 0 to 3.0240e-03.\n",
      "Epoch 00969: reducing learning rate of group 0 to 2.9938e-03.\n",
      "Epoch 00975: reducing learning rate of group 0 to 2.9639e-03.\n",
      "Epoch 00981: reducing learning rate of group 0 to 2.9342e-03.\n",
      "Epoch 00987: reducing learning rate of group 0 to 2.9049e-03.\n",
      "Epoch 00993: reducing learning rate of group 0 to 2.8758e-03.\n",
      "Epoch 00999: reducing learning rate of group 0 to 2.8471e-03.\n",
      "[10m 38s (1000 20%) train loss: 1.8108, test_loss: 1.9267]\n",
      "Whoumlat for thes ake agame as my worsen ward a stom to like wiulbE\n",
      "The  wal\n",
      "ie to uodO thee mane till \n",
      "\n",
      "Epoch 01005: reducing learning rate of group 0 to 2.8186e-03.\n",
      "Epoch 01011: reducing learning rate of group 0 to 2.7904e-03.\n",
      "Epoch 01017: reducing learning rate of group 0 to 2.7625e-03.\n",
      "Epoch 01023: reducing learning rate of group 0 to 2.7349e-03.\n",
      "Epoch 01029: reducing learning rate of group 0 to 2.7075e-03.\n",
      "Epoch 01035: reducing learning rate of group 0 to 2.6805e-03.\n",
      "Epoch 01041: reducing learning rate of group 0 to 2.6537e-03.\n",
      "Epoch 01047: reducing learning rate of group 0 to 2.6271e-03.\n",
      "Epoch 01054: reducing learning rate of group 0 to 2.6009e-03.\n",
      "Epoch 01060: reducing learning rate of group 0 to 2.5748e-03.\n",
      "Epoch 01066: reducing learning rate of group 0 to 2.5491e-03.\n",
      "Epoch 01072: reducing learning rate of group 0 to 2.5236e-03.\n",
      "Epoch 01078: reducing learning rate of group 0 to 2.4984e-03.\n",
      "Epoch 01084: reducing learning rate of group 0 to 2.4734e-03.\n",
      "Epoch 01090: reducing learning rate of group 0 to 2.4487e-03.\n",
      "Epoch 01099: reducing learning rate of group 0 to 2.4242e-03.\n",
      "[11m 41s (1100 22%) train loss: 1.7961, test_loss: 1.9035]\n",
      "Whaar that i kne'mr, in lost youk his hisredrend a tomes aer forto should this to the comeous in the!  \n",
      "\n",
      "Epoch 01105: reducing learning rate of group 0 to 2.3999e-03.\n",
      "Epoch 01111: reducing learning rate of group 0 to 2.3759e-03.\n",
      "Epoch 01117: reducing learning rate of group 0 to 2.3522e-03.\n",
      "Epoch 01123: reducing learning rate of group 0 to 2.3286e-03.\n",
      "Epoch 01129: reducing learning rate of group 0 to 2.3054e-03.\n",
      "Epoch 01135: reducing learning rate of group 0 to 2.2823e-03.\n",
      "Epoch 01141: reducing learning rate of group 0 to 2.2595e-03.\n",
      "Epoch 01147: reducing learning rate of group 0 to 2.2369e-03.\n",
      "Epoch 01153: reducing learning rate of group 0 to 2.2145e-03.\n",
      "Epoch 01159: reducing learning rate of group 0 to 2.1924e-03.\n",
      "Epoch 01165: reducing learning rate of group 0 to 2.1704e-03.\n",
      "Epoch 01171: reducing learning rate of group 0 to 2.1487e-03.\n",
      "Epoch 01177: reducing learning rate of group 0 to 2.1273e-03.\n",
      "Epoch 01183: reducing learning rate of group 0 to 2.1060e-03.\n",
      "Epoch 01189: reducing learning rate of group 0 to 2.0849e-03.\n",
      "Epoch 01195: reducing learning rate of group 0 to 2.0641e-03.\n",
      "[12m 45s (1200 24%) train loss: 1.8022, test_loss: 1.9263]\n",
      "What the forliser, for his acile as his will his hands, for maningreel and hiry wordlesser parting Gwo \n",
      "\n",
      "Epoch 01201: reducing learning rate of group 0 to 2.0434e-03.\n",
      "Epoch 01209: reducing learning rate of group 0 to 2.0230e-03.\n",
      "Epoch 01215: reducing learning rate of group 0 to 2.0028e-03.\n",
      "Epoch 01221: reducing learning rate of group 0 to 1.9827e-03.\n",
      "Epoch 01227: reducing learning rate of group 0 to 1.9629e-03.\n",
      "Epoch 01233: reducing learning rate of group 0 to 1.9433e-03.\n",
      "Epoch 01239: reducing learning rate of group 0 to 1.9239e-03.\n",
      "Epoch 01247: reducing learning rate of group 0 to 1.9046e-03.\n",
      "Epoch 01253: reducing learning rate of group 0 to 1.8856e-03.\n",
      "Epoch 01259: reducing learning rate of group 0 to 1.8667e-03.\n",
      "Epoch 01269: reducing learning rate of group 0 to 1.8480e-03.\n",
      "Epoch 01275: reducing learning rate of group 0 to 1.8296e-03.\n",
      "Epoch 01281: reducing learning rate of group 0 to 1.8113e-03.\n",
      "Epoch 01287: reducing learning rate of group 0 to 1.7932e-03.\n",
      "Epoch 01293: reducing learning rate of group 0 to 1.7752e-03.\n",
      "Epoch 01299: reducing learning rate of group 0 to 1.7575e-03.\n",
      "[13m 48s (1300 26%) train loss: 1.7864, test_loss: 1.9131]\n",
      "Who do with sa, in stard the reverst upone, reence though thou come conce that nothing with thinmeNt m \n",
      "\n",
      "Epoch 01305: reducing learning rate of group 0 to 1.7399e-03.\n",
      "Epoch 01311: reducing learning rate of group 0 to 1.7225e-03.\n",
      "Epoch 01317: reducing learning rate of group 0 to 1.7053e-03.\n",
      "Epoch 01323: reducing learning rate of group 0 to 1.6882e-03.\n",
      "Epoch 01329: reducing learning rate of group 0 to 1.6713e-03.\n",
      "Epoch 01335: reducing learning rate of group 0 to 1.6546e-03.\n",
      "Epoch 01341: reducing learning rate of group 0 to 1.6381e-03.\n",
      "Epoch 01347: reducing learning rate of group 0 to 1.6217e-03.\n",
      "Epoch 01353: reducing learning rate of group 0 to 1.6055e-03.\n",
      "Epoch 01359: reducing learning rate of group 0 to 1.5894e-03.\n",
      "Epoch 01365: reducing learning rate of group 0 to 1.5735e-03.\n",
      "Epoch 01371: reducing learning rate of group 0 to 1.5578e-03.\n",
      "Epoch 01377: reducing learning rate of group 0 to 1.5422e-03.\n",
      "Epoch 01383: reducing learning rate of group 0 to 1.5268e-03.\n",
      "Epoch 01389: reducing learning rate of group 0 to 1.5115e-03.\n",
      "Epoch 01395: reducing learning rate of group 0 to 1.4964e-03.\n",
      "[14m 52s (1400 28%) train loss: 1.7814, test_loss: 1.8991]\n",
      "Whant but wars to my sorcous.\n",
      "\n",
      "HENRicrls:\n",
      "le me sare not fore nor that notGU will heyNong, theY to de  \n",
      "\n",
      "Epoch 01401: reducing learning rate of group 0 to 1.4814e-03.\n",
      "Epoch 01407: reducing learning rate of group 0 to 1.4666e-03.\n",
      "Epoch 01413: reducing learning rate of group 0 to 1.4520e-03.\n",
      "Epoch 01419: reducing learning rate of group 0 to 1.4374e-03.\n",
      "Epoch 01431: reducing learning rate of group 0 to 1.4231e-03.\n",
      "Epoch 01437: reducing learning rate of group 0 to 1.4088e-03.\n",
      "Epoch 01445: reducing learning rate of group 0 to 1.3948e-03.\n",
      "Epoch 01451: reducing learning rate of group 0 to 1.3808e-03.\n",
      "Epoch 01457: reducing learning rate of group 0 to 1.3670e-03.\n",
      "Epoch 01463: reducing learning rate of group 0 to 1.3533e-03.\n",
      "Epoch 01469: reducing learning rate of group 0 to 1.3398e-03.\n",
      "Epoch 01475: reducing learning rate of group 0 to 1.3264e-03.\n",
      "Epoch 01481: reducing learning rate of group 0 to 1.3131e-03.\n",
      "Epoch 01487: reducing learning rate of group 0 to 1.3000e-03.\n",
      "Epoch 01493: reducing learning rate of group 0 to 1.2870e-03.\n",
      "Epoch 01499: reducing learning rate of group 0 to 1.2741e-03.\n",
      "[15m 57s (1500 30%) train loss: 1.7740, test_loss: 1.8897]\n",
      "Whear gave be thee in this apperJuratles of the Singes stal ho these do my rtanulisese and becrowing L \n",
      "\n",
      "Epoch 01505: reducing learning rate of group 0 to 1.2614e-03.\n",
      "Epoch 01511: reducing learning rate of group 0 to 1.2488e-03.\n",
      "Epoch 01517: reducing learning rate of group 0 to 1.2363e-03.\n",
      "Epoch 01523: reducing learning rate of group 0 to 1.2239e-03.\n",
      "Epoch 01529: reducing learning rate of group 0 to 1.2117e-03.\n",
      "Epoch 01535: reducing learning rate of group 0 to 1.1996e-03.\n",
      "Epoch 01541: reducing learning rate of group 0 to 1.1876e-03.\n",
      "Epoch 01547: reducing learning rate of group 0 to 1.1757e-03.\n",
      "Epoch 01553: reducing learning rate of group 0 to 1.1639e-03.\n",
      "Epoch 01559: reducing learning rate of group 0 to 1.1523e-03.\n",
      "Epoch 01565: reducing learning rate of group 0 to 1.1408e-03.\n",
      "Epoch 01571: reducing learning rate of group 0 to 1.1294e-03.\n",
      "Epoch 01577: reducing learning rate of group 0 to 1.1181e-03.\n",
      "Epoch 01583: reducing learning rate of group 0 to 1.1069e-03.\n",
      "Epoch 01589: reducing learning rate of group 0 to 1.0958e-03.\n",
      "Epoch 01595: reducing learning rate of group 0 to 1.0849e-03.\n",
      "[17m 0s (1600 32%) train loss: 1.7751, test_loss: 1.8845]\n",
      "What se a toldi:\n",
      "Shea with his wity to the handeerd on with fathers drames Tooe the orse your himer\n",
      "th \n",
      "\n",
      "Epoch 01601: reducing learning rate of group 0 to 1.0740e-03.\n",
      "Epoch 01607: reducing learning rate of group 0 to 1.0633e-03.\n",
      "Epoch 01613: reducing learning rate of group 0 to 1.0526e-03.\n",
      "Epoch 01619: reducing learning rate of group 0 to 1.0421e-03.\n",
      "Epoch 01625: reducing learning rate of group 0 to 1.0317e-03.\n",
      "Epoch 01631: reducing learning rate of group 0 to 1.0214e-03.\n",
      "Epoch 01637: reducing learning rate of group 0 to 1.0112e-03.\n",
      "Epoch 01645: reducing learning rate of group 0 to 1.0011e-03.\n",
      "Epoch 01651: reducing learning rate of group 0 to 9.9105e-04.\n",
      "Epoch 01657: reducing learning rate of group 0 to 9.8114e-04.\n",
      "Epoch 01663: reducing learning rate of group 0 to 9.7133e-04.\n",
      "Epoch 01669: reducing learning rate of group 0 to 9.6161e-04.\n",
      "Epoch 01675: reducing learning rate of group 0 to 9.5200e-04.\n",
      "Epoch 01681: reducing learning rate of group 0 to 9.4248e-04.\n",
      "Epoch 01687: reducing learning rate of group 0 to 9.3305e-04.\n",
      "Epoch 01693: reducing learning rate of group 0 to 9.2372e-04.\n",
      "Epoch 01699: reducing learning rate of group 0 to 9.1448e-04.\n",
      "[18m 4s (1700 34%) train loss: 1.7668, test_loss: 1.8932]\n",
      "Who as the cary ass ighting perasted ausordurne\n",
      "torblous in it\n",
      "The as the Murdery\n",
      "Wind Henlerenerrs\n",
      "Wi \n",
      "\n",
      "Epoch 01705: reducing learning rate of group 0 to 9.0534e-04.\n",
      "Epoch 01716: reducing learning rate of group 0 to 8.9629e-04.\n",
      "Epoch 01722: reducing learning rate of group 0 to 8.8732e-04.\n",
      "Epoch 01728: reducing learning rate of group 0 to 8.7845e-04.\n",
      "Epoch 01734: reducing learning rate of group 0 to 8.6967e-04.\n",
      "Epoch 01740: reducing learning rate of group 0 to 8.6097e-04.\n",
      "Epoch 01746: reducing learning rate of group 0 to 8.5236e-04.\n",
      "Epoch 01752: reducing learning rate of group 0 to 8.4384e-04.\n",
      "Epoch 01758: reducing learning rate of group 0 to 8.3540e-04.\n",
      "Epoch 01764: reducing learning rate of group 0 to 8.2704e-04.\n",
      "Epoch 01770: reducing learning rate of group 0 to 8.1877e-04.\n",
      "Epoch 01776: reducing learning rate of group 0 to 8.1059e-04.\n",
      "Epoch 01782: reducing learning rate of group 0 to 8.0248e-04.\n",
      "Epoch 01788: reducing learning rate of group 0 to 7.9445e-04.\n",
      "Epoch 01794: reducing learning rate of group 0 to 7.8651e-04.\n",
      "Epoch 01800: reducing learning rate of group 0 to 7.7864e-04.\n",
      "[19m 7s (1800 36%) train loss: 1.7826, test_loss: 1.8866]\n",
      "Whiers, whishing Marer of gremerigle\n",
      "\n",
      "aloma: for beet handd'se \n",
      "\n",
      "Fast and fortures whears to bruspendo \n",
      "\n",
      "Epoch 01806: reducing learning rate of group 0 to 7.7086e-04.\n",
      "Epoch 01812: reducing learning rate of group 0 to 7.6315e-04.\n",
      "Epoch 01818: reducing learning rate of group 0 to 7.5552e-04.\n",
      "Epoch 01824: reducing learning rate of group 0 to 7.4796e-04.\n",
      "Epoch 01830: reducing learning rate of group 0 to 7.4048e-04.\n",
      "Epoch 01836: reducing learning rate of group 0 to 7.3308e-04.\n",
      "Epoch 01842: reducing learning rate of group 0 to 7.2575e-04.\n",
      "Epoch 01848: reducing learning rate of group 0 to 7.1849e-04.\n",
      "Epoch 01854: reducing learning rate of group 0 to 7.1131e-04.\n",
      "Epoch 01860: reducing learning rate of group 0 to 7.0419e-04.\n",
      "Epoch 01866: reducing learning rate of group 0 to 6.9715e-04.\n",
      "Epoch 01872: reducing learning rate of group 0 to 6.9018e-04.\n",
      "Epoch 01878: reducing learning rate of group 0 to 6.8328e-04.\n",
      "Epoch 01884: reducing learning rate of group 0 to 6.7644e-04.\n",
      "Epoch 01890: reducing learning rate of group 0 to 6.6968e-04.\n",
      "Epoch 01896: reducing learning rate of group 0 to 6.6298e-04.\n",
      "[20m 10s (1900 38%) train loss: 1.7759, test_loss: 1.8870]\n",
      "What lies of gooes thilt for me your more share he as the may whoughting the nwel?\n",
      "\n",
      "ROMNEUS:\n",
      "And come  \n",
      "\n",
      "Epoch 01902: reducing learning rate of group 0 to 6.5635e-04.\n",
      "Epoch 01908: reducing learning rate of group 0 to 6.4979e-04.\n",
      "Epoch 01914: reducing learning rate of group 0 to 6.4329e-04.\n",
      "Epoch 01920: reducing learning rate of group 0 to 6.3686e-04.\n",
      "Epoch 01926: reducing learning rate of group 0 to 6.3049e-04.\n",
      "Epoch 01932: reducing learning rate of group 0 to 6.2419e-04.\n",
      "Epoch 01938: reducing learning rate of group 0 to 6.1794e-04.\n",
      "Epoch 01944: reducing learning rate of group 0 to 6.1176e-04.\n",
      "Epoch 01950: reducing learning rate of group 0 to 6.0565e-04.\n",
      "Epoch 01956: reducing learning rate of group 0 to 5.9959e-04.\n",
      "Epoch 01962: reducing learning rate of group 0 to 5.9359e-04.\n",
      "Epoch 01968: reducing learning rate of group 0 to 5.8766e-04.\n",
      "Epoch 01974: reducing learning rate of group 0 to 5.8178e-04.\n",
      "Epoch 01980: reducing learning rate of group 0 to 5.7596e-04.\n",
      "Epoch 01986: reducing learning rate of group 0 to 5.7020e-04.\n",
      "Epoch 01992: reducing learning rate of group 0 to 5.6450e-04.\n",
      "Epoch 01998: reducing learning rate of group 0 to 5.5886e-04.\n",
      "[21m 15s (2000 40%) train loss: 1.7661, test_loss: 1.8884]\n",
      "Wher well merelly ition and unat,\n",
      "My toor the other manp\n",
      "alle and drsinging the care of theughten we s \n",
      "\n",
      "Epoch 02004: reducing learning rate of group 0 to 5.5327e-04.\n",
      "Epoch 02010: reducing learning rate of group 0 to 5.4774e-04.\n",
      "Epoch 02016: reducing learning rate of group 0 to 5.4226e-04.\n",
      "Epoch 02022: reducing learning rate of group 0 to 5.3684e-04.\n",
      "Epoch 02028: reducing learning rate of group 0 to 5.3147e-04.\n",
      "Epoch 02034: reducing learning rate of group 0 to 5.2615e-04.\n",
      "Epoch 02040: reducing learning rate of group 0 to 5.2089e-04.\n",
      "Epoch 02046: reducing learning rate of group 0 to 5.1568e-04.\n",
      "Epoch 02052: reducing learning rate of group 0 to 5.1053e-04.\n",
      "Epoch 02058: reducing learning rate of group 0 to 5.0542e-04.\n",
      "Epoch 02064: reducing learning rate of group 0 to 5.0037e-04.\n",
      "Epoch 02070: reducing learning rate of group 0 to 4.9536e-04.\n",
      "Epoch 02076: reducing learning rate of group 0 to 4.9041e-04.\n",
      "Epoch 02082: reducing learning rate of group 0 to 4.8550e-04.\n",
      "Epoch 02088: reducing learning rate of group 0 to 4.8065e-04.\n",
      "Epoch 02094: reducing learning rate of group 0 to 4.7584e-04.\n",
      "Epoch 02100: reducing learning rate of group 0 to 4.7108e-04.\n",
      "[22m 17s (2100 42%) train loss: 1.7518, test_loss: 1.9002]\n",
      "Where to be ploterence in I and when I\n",
      "Re.\n",
      "in the meaf is ad and Bafdier, with the preand,\n",
      "When Rome\n",
      "T \n",
      "\n",
      "Epoch 02106: reducing learning rate of group 0 to 4.6637e-04.\n",
      "Epoch 02112: reducing learning rate of group 0 to 4.6171e-04.\n",
      "Epoch 02118: reducing learning rate of group 0 to 4.5709e-04.\n",
      "Epoch 02124: reducing learning rate of group 0 to 4.5252e-04.\n",
      "Epoch 02130: reducing learning rate of group 0 to 4.4800e-04.\n",
      "Epoch 02136: reducing learning rate of group 0 to 4.4352e-04.\n",
      "Epoch 02142: reducing learning rate of group 0 to 4.3908e-04.\n",
      "Epoch 02148: reducing learning rate of group 0 to 4.3469e-04.\n",
      "Epoch 02154: reducing learning rate of group 0 to 4.3034e-04.\n",
      "Epoch 02160: reducing learning rate of group 0 to 4.2604e-04.\n",
      "Epoch 02166: reducing learning rate of group 0 to 4.2178e-04.\n",
      "Epoch 02172: reducing learning rate of group 0 to 4.1756e-04.\n",
      "Epoch 02178: reducing learning rate of group 0 to 4.1339e-04.\n",
      "Epoch 02184: reducing learning rate of group 0 to 4.0925e-04.\n",
      "Epoch 02190: reducing learning rate of group 0 to 4.0516e-04.\n",
      "Epoch 02196: reducing learning rate of group 0 to 4.0111e-04.\n",
      "[23m 21s (2200 44%) train loss: 1.7678, test_loss: 1.9085]\n",
      "Who soul learven paunen s olds brother, broble this comp not the comes hd\n",
      "enter my beed the commit! no \n",
      "\n",
      "Epoch 02202: reducing learning rate of group 0 to 3.9710e-04.\n",
      "Epoch 02208: reducing learning rate of group 0 to 3.9313e-04.\n",
      "Epoch 02214: reducing learning rate of group 0 to 3.8920e-04.\n",
      "Epoch 02220: reducing learning rate of group 0 to 3.8530e-04.\n",
      "Epoch 02226: reducing learning rate of group 0 to 3.8145e-04.\n",
      "Epoch 02232: reducing learning rate of group 0 to 3.7764e-04.\n",
      "Epoch 02238: reducing learning rate of group 0 to 3.7386e-04.\n",
      "Epoch 02244: reducing learning rate of group 0 to 3.7012e-04.\n",
      "Epoch 02250: reducing learning rate of group 0 to 3.6642e-04.\n",
      "Epoch 02256: reducing learning rate of group 0 to 3.6276e-04.\n",
      "Epoch 02262: reducing learning rate of group 0 to 3.5913e-04.\n",
      "Epoch 02268: reducing learning rate of group 0 to 3.5554e-04.\n",
      "Epoch 02274: reducing learning rate of group 0 to 3.5198e-04.\n",
      "Epoch 02280: reducing learning rate of group 0 to 3.4846e-04.\n",
      "Epoch 02286: reducing learning rate of group 0 to 3.4498e-04.\n",
      "Epoch 02292: reducing learning rate of group 0 to 3.4153e-04.\n",
      "Epoch 02298: reducing learning rate of group 0 to 3.3811e-04.\n",
      "[24m 24s (2300 46%) train loss: 1.7737, test_loss: 1.8831]\n",
      "Whad thee of my mother ray my simen wet.\n",
      "\n",
      "LULIvarO:\n",
      "The mauntcer death to her tunen her than kuni's re \n",
      "\n",
      "Epoch 02304: reducing learning rate of group 0 to 3.3473e-04.\n",
      "Epoch 02310: reducing learning rate of group 0 to 3.3138e-04.\n",
      "Epoch 02316: reducing learning rate of group 0 to 3.2807e-04.\n",
      "Epoch 02322: reducing learning rate of group 0 to 3.2479e-04.\n",
      "Epoch 02328: reducing learning rate of group 0 to 3.2154e-04.\n",
      "Epoch 02334: reducing learning rate of group 0 to 3.1833e-04.\n",
      "Epoch 02340: reducing learning rate of group 0 to 3.1514e-04.\n",
      "Epoch 02346: reducing learning rate of group 0 to 3.1199e-04.\n",
      "Epoch 02352: reducing learning rate of group 0 to 3.0887e-04.\n",
      "Epoch 02363: reducing learning rate of group 0 to 3.0578e-04.\n",
      "Epoch 02369: reducing learning rate of group 0 to 3.0272e-04.\n",
      "Epoch 02375: reducing learning rate of group 0 to 2.9970e-04.\n",
      "Epoch 02381: reducing learning rate of group 0 to 2.9670e-04.\n",
      "Epoch 02387: reducing learning rate of group 0 to 2.9373e-04.\n",
      "Epoch 02393: reducing learning rate of group 0 to 2.9080e-04.\n",
      "Epoch 02399: reducing learning rate of group 0 to 2.8789e-04.\n",
      "[25m 28s (2400 48%) train loss: 1.7585, test_loss: 1.8748]\n",
      "What statest; be you son till the hacint stact and this stath notlefding the tib.\n",
      " oay shands thex our \n",
      "\n",
      "Epoch 02405: reducing learning rate of group 0 to 2.8501e-04.\n",
      "Epoch 02411: reducing learning rate of group 0 to 2.8216e-04.\n",
      "Epoch 02417: reducing learning rate of group 0 to 2.7934e-04.\n",
      "Epoch 02423: reducing learning rate of group 0 to 2.7654e-04.\n",
      "Epoch 02429: reducing learning rate of group 0 to 2.7378e-04.\n",
      "Epoch 02435: reducing learning rate of group 0 to 2.7104e-04.\n",
      "Epoch 02441: reducing learning rate of group 0 to 2.6833e-04.\n",
      "Epoch 02447: reducing learning rate of group 0 to 2.6565e-04.\n",
      "Epoch 02453: reducing learning rate of group 0 to 2.6299e-04.\n",
      "Epoch 02459: reducing learning rate of group 0 to 2.6036e-04.\n",
      "Epoch 02465: reducing learning rate of group 0 to 2.5776e-04.\n",
      "Epoch 02471: reducing learning rate of group 0 to 2.5518e-04.\n",
      "Epoch 02477: reducing learning rate of group 0 to 2.5263e-04.\n",
      "Epoch 02483: reducing learning rate of group 0 to 2.5010e-04.\n",
      "Epoch 02489: reducing learning rate of group 0 to 2.4760e-04.\n",
      "Epoch 02495: reducing learning rate of group 0 to 2.4512e-04.\n",
      "[26m 32s (2500 50%) train loss: 1.7623, test_loss: 1.8938]\n",
      "Why howrcea his whact hard, all dind\n",
      "Low her seeo, you couster who enat others of the that he momfropp \n",
      "\n",
      "Epoch 02501: reducing learning rate of group 0 to 2.4267e-04.\n",
      "Epoch 02507: reducing learning rate of group 0 to 2.4025e-04.\n",
      "Epoch 02513: reducing learning rate of group 0 to 2.3784e-04.\n",
      "Epoch 02519: reducing learning rate of group 0 to 2.3547e-04.\n",
      "Epoch 02525: reducing learning rate of group 0 to 2.3311e-04.\n",
      "Epoch 02531: reducing learning rate of group 0 to 2.3078e-04.\n",
      "Epoch 02537: reducing learning rate of group 0 to 2.2847e-04.\n",
      "Epoch 02543: reducing learning rate of group 0 to 2.2619e-04.\n",
      "Epoch 02549: reducing learning rate of group 0 to 2.2393e-04.\n",
      "Epoch 02555: reducing learning rate of group 0 to 2.2169e-04.\n",
      "Epoch 02561: reducing learning rate of group 0 to 2.1947e-04.\n",
      "Epoch 02567: reducing learning rate of group 0 to 2.1727e-04.\n",
      "Epoch 02573: reducing learning rate of group 0 to 2.1510e-04.\n",
      "Epoch 02579: reducing learning rate of group 0 to 2.1295e-04.\n",
      "Epoch 02585: reducing learning rate of group 0 to 2.1082e-04.\n",
      "Epoch 02591: reducing learning rate of group 0 to 2.0871e-04.\n",
      "Epoch 02597: reducing learning rate of group 0 to 2.0663e-04.\n",
      "[27m 36s (2600 52%) train loss: 1.7552, test_loss: 1.8833]\n",
      "What didew, him doon to wey of ip the door of miyh is manfofMom were to thee my lord:\n",
      "And this pamby l \n",
      "\n",
      "Epoch 02603: reducing learning rate of group 0 to 2.0456e-04.\n",
      "Epoch 02609: reducing learning rate of group 0 to 2.0251e-04.\n",
      "Epoch 02615: reducing learning rate of group 0 to 2.0049e-04.\n",
      "Epoch 02624: reducing learning rate of group 0 to 1.9848e-04.\n",
      "Epoch 02630: reducing learning rate of group 0 to 1.9650e-04.\n",
      "Epoch 02636: reducing learning rate of group 0 to 1.9453e-04.\n",
      "Epoch 02642: reducing learning rate of group 0 to 1.9259e-04.\n",
      "Epoch 02648: reducing learning rate of group 0 to 1.9066e-04.\n",
      "Epoch 02654: reducing learning rate of group 0 to 1.8876e-04.\n",
      "Epoch 02660: reducing learning rate of group 0 to 1.8687e-04.\n",
      "Epoch 02666: reducing learning rate of group 0 to 1.8500e-04.\n",
      "Epoch 02672: reducing learning rate of group 0 to 1.8315e-04.\n",
      "Epoch 02678: reducing learning rate of group 0 to 1.8132e-04.\n",
      "Epoch 02684: reducing learning rate of group 0 to 1.7951e-04.\n",
      "Epoch 02690: reducing learning rate of group 0 to 1.7771e-04.\n",
      "Epoch 02696: reducing learning rate of group 0 to 1.7593e-04.\n",
      "[28m 40s (2700 54%) train loss: 1.7546, test_loss: 1.8907]\n",
      "Why ay, I  honO with her wilts there and couture of fis to  for be my come the shat:\n",
      "'Tis kno my toor  \n",
      "\n",
      "Epoch 02702: reducing learning rate of group 0 to 1.7417e-04.\n",
      "Epoch 02708: reducing learning rate of group 0 to 1.7243e-04.\n",
      "Epoch 02714: reducing learning rate of group 0 to 1.7071e-04.\n",
      "Epoch 02720: reducing learning rate of group 0 to 1.6900e-04.\n",
      "Epoch 02726: reducing learning rate of group 0 to 1.6731e-04.\n",
      "Epoch 02732: reducing learning rate of group 0 to 1.6564e-04.\n",
      "Epoch 02738: reducing learning rate of group 0 to 1.6398e-04.\n",
      "Epoch 02744: reducing learning rate of group 0 to 1.6234e-04.\n",
      "Epoch 02750: reducing learning rate of group 0 to 1.6072e-04.\n",
      "Epoch 02756: reducing learning rate of group 0 to 1.5911e-04.\n",
      "Epoch 02762: reducing learning rate of group 0 to 1.5752e-04.\n",
      "Epoch 02768: reducing learning rate of group 0 to 1.5594e-04.\n",
      "Epoch 02774: reducing learning rate of group 0 to 1.5439e-04.\n",
      "Epoch 02780: reducing learning rate of group 0 to 1.5284e-04.\n",
      "Epoch 02786: reducing learning rate of group 0 to 1.5131e-04.\n",
      "Epoch 02792: reducing learning rate of group 0 to 1.4980e-04.\n",
      "Epoch 02798: reducing learning rate of group 0 to 1.4830e-04.\n",
      "[29m 43s (2800 56%) train loss: 1.7579, test_loss: 1.8745]\n",
      "Where the masters,\n",
      "Thet and downs\n",
      "See, I \n",
      "plingemsing.\n",
      "I with hip o to the is my this lmord sunmence h \n",
      "\n",
      "Epoch 02804: reducing learning rate of group 0 to 1.4682e-04.\n",
      "Epoch 02810: reducing learning rate of group 0 to 1.4535e-04.\n",
      "Epoch 02816: reducing learning rate of group 0 to 1.4390e-04.\n",
      "Epoch 02822: reducing learning rate of group 0 to 1.4246e-04.\n",
      "Epoch 02828: reducing learning rate of group 0 to 1.4103e-04.\n",
      "Epoch 02834: reducing learning rate of group 0 to 1.3962e-04.\n",
      "Epoch 02840: reducing learning rate of group 0 to 1.3823e-04.\n",
      "Epoch 02846: reducing learning rate of group 0 to 1.3684e-04.\n",
      "Epoch 02852: reducing learning rate of group 0 to 1.3548e-04.\n",
      "Epoch 02858: reducing learning rate of group 0 to 1.3412e-04.\n",
      "Epoch 02864: reducing learning rate of group 0 to 1.3278e-04.\n",
      "Epoch 02870: reducing learning rate of group 0 to 1.3145e-04.\n",
      "Epoch 02876: reducing learning rate of group 0 to 1.3014e-04.\n",
      "Epoch 02882: reducing learning rate of group 0 to 1.2884e-04.\n",
      "Epoch 02888: reducing learning rate of group 0 to 1.2755e-04.\n",
      "Epoch 02894: reducing learning rate of group 0 to 1.2627e-04.\n",
      "Epoch 02900: reducing learning rate of group 0 to 1.2501e-04.\n",
      "[30m 46s (2900 57%) train loss: 1.7566, test_loss: 1.8669]\n",
      "Which wiment, to lix Catking so deteerty of me, the everer that deaves it is siolane more spear hi.\n",
      "\n",
      "M \n",
      "\n",
      "Epoch 02906: reducing learning rate of group 0 to 1.2376e-04.\n",
      "Epoch 02912: reducing learning rate of group 0 to 1.2252e-04.\n",
      "Epoch 02918: reducing learning rate of group 0 to 1.2130e-04.\n",
      "Epoch 02924: reducing learning rate of group 0 to 1.2008e-04.\n",
      "Epoch 02930: reducing learning rate of group 0 to 1.1888e-04.\n",
      "Epoch 02936: reducing learning rate of group 0 to 1.1769e-04.\n",
      "Epoch 02942: reducing learning rate of group 0 to 1.1652e-04.\n",
      "Epoch 02948: reducing learning rate of group 0 to 1.1535e-04.\n",
      "Epoch 02954: reducing learning rate of group 0 to 1.1420e-04.\n",
      "Epoch 02960: reducing learning rate of group 0 to 1.1306e-04.\n",
      "Epoch 02966: reducing learning rate of group 0 to 1.1193e-04.\n",
      "Epoch 02972: reducing learning rate of group 0 to 1.1081e-04.\n",
      "Epoch 02978: reducing learning rate of group 0 to 1.0970e-04.\n",
      "Epoch 02984: reducing learning rate of group 0 to 1.0860e-04.\n",
      "Epoch 02990: reducing learning rate of group 0 to 1.0752e-04.\n",
      "Epoch 02996: reducing learning rate of group 0 to 1.0644e-04.\n",
      "[31m 51s (3000 60%) train loss: 1.7822, test_loss: 1.8981]\n",
      "Which'y,\n",
      "She am Serne shall call you !iin mofing but to a st to my old be come's shaI\n",
      "Brath Thencome h \n",
      "\n",
      "Epoch 03002: reducing learning rate of group 0 to 1.0538e-04.\n",
      "Epoch 03008: reducing learning rate of group 0 to 1.0432e-04.\n",
      "Epoch 03014: reducing learning rate of group 0 to 1.0328e-04.\n",
      "Epoch 03020: reducing learning rate of group 0 to 1.0225e-04.\n",
      "Epoch 03026: reducing learning rate of group 0 to 1.0122e-04.\n",
      "Epoch 03032: reducing learning rate of group 0 to 1.0021e-04.\n",
      "Epoch 03038: reducing learning rate of group 0 to 9.9210e-05.\n",
      "Epoch 03044: reducing learning rate of group 0 to 9.8218e-05.\n",
      "Epoch 03050: reducing learning rate of group 0 to 9.7235e-05.\n",
      "Epoch 03056: reducing learning rate of group 0 to 9.6263e-05.\n",
      "Epoch 03062: reducing learning rate of group 0 to 9.5300e-05.\n",
      "Epoch 03068: reducing learning rate of group 0 to 9.4347e-05.\n",
      "Epoch 03074: reducing learning rate of group 0 to 9.3404e-05.\n",
      "Epoch 03080: reducing learning rate of group 0 to 9.2470e-05.\n",
      "Epoch 03086: reducing learning rate of group 0 to 9.1545e-05.\n",
      "Epoch 03092: reducing learning rate of group 0 to 9.0630e-05.\n",
      "Epoch 03098: reducing learning rate of group 0 to 8.9724e-05.\n",
      "[32m 55s (3100 62%) train loss: 1.7588, test_loss: 1.8786]\n",
      "Wher be strange wored the worls,\n",
      "Treantencelmat out that bed agalling to the frare for I caet to his t \n",
      "\n",
      "Epoch 03104: reducing learning rate of group 0 to 8.8826e-05.\n",
      "Epoch 03110: reducing learning rate of group 0 to 8.7938e-05.\n",
      "Epoch 03116: reducing learning rate of group 0 to 8.7059e-05.\n",
      "Epoch 03122: reducing learning rate of group 0 to 8.6188e-05.\n",
      "Epoch 03128: reducing learning rate of group 0 to 8.5326e-05.\n",
      "Epoch 03134: reducing learning rate of group 0 to 8.4473e-05.\n",
      "Epoch 03140: reducing learning rate of group 0 to 8.3628e-05.\n",
      "Epoch 03146: reducing learning rate of group 0 to 8.2792e-05.\n",
      "Epoch 03152: reducing learning rate of group 0 to 8.1964e-05.\n",
      "Epoch 03158: reducing learning rate of group 0 to 8.1144e-05.\n",
      "Epoch 03164: reducing learning rate of group 0 to 8.0333e-05.\n",
      "Epoch 03170: reducing learning rate of group 0 to 7.9530e-05.\n",
      "Epoch 03176: reducing learning rate of group 0 to 7.8734e-05.\n",
      "Epoch 03182: reducing learning rate of group 0 to 7.7947e-05.\n",
      "Epoch 03188: reducing learning rate of group 0 to 7.7167e-05.\n",
      "Epoch 03194: reducing learning rate of group 0 to 7.6396e-05.\n",
      "Epoch 03200: reducing learning rate of group 0 to 7.5632e-05.\n",
      "[33m 59s (3200 64%) train loss: 1.7530, test_loss: 1.8813]\n",
      "What thine know\n",
      "Anto the vater.\n",
      "\n",
      "Pordis, he molk theother Clranderst thispale this uncalGund take the  \n",
      "\n",
      "Epoch 03206: reducing learning rate of group 0 to 7.4876e-05.\n",
      "Epoch 03212: reducing learning rate of group 0 to 7.4127e-05.\n",
      "Epoch 03218: reducing learning rate of group 0 to 7.3385e-05.\n",
      "Epoch 03224: reducing learning rate of group 0 to 7.2652e-05.\n",
      "Epoch 03230: reducing learning rate of group 0 to 7.1925e-05.\n",
      "Epoch 03236: reducing learning rate of group 0 to 7.1206e-05.\n",
      "Epoch 03242: reducing learning rate of group 0 to 7.0494e-05.\n",
      "Epoch 03248: reducing learning rate of group 0 to 6.9789e-05.\n",
      "Epoch 03254: reducing learning rate of group 0 to 6.9091e-05.\n",
      "Epoch 03260: reducing learning rate of group 0 to 6.8400e-05.\n",
      "Epoch 03266: reducing learning rate of group 0 to 6.7716e-05.\n",
      "Epoch 03272: reducing learning rate of group 0 to 6.7039e-05.\n",
      "Epoch 03278: reducing learning rate of group 0 to 6.6369e-05.\n",
      "Epoch 03284: reducing learning rate of group 0 to 6.5705e-05.\n",
      "Epoch 03290: reducing learning rate of group 0 to 6.5048e-05.\n",
      "Epoch 03296: reducing learning rate of group 0 to 6.4397e-05.\n",
      "[35m 2s (3300 66%) train loss: 1.7583, test_loss: 1.8827]\n",
      "Whal thwence, oy you endcord in the god, and what are him dir,\n",
      "o\n",
      "Thoul means that wut on thepersen bla \n",
      "\n",
      "Epoch 03302: reducing learning rate of group 0 to 6.3753e-05.\n",
      "Epoch 03308: reducing learning rate of group 0 to 6.3116e-05.\n",
      "Epoch 03314: reducing learning rate of group 0 to 6.2485e-05.\n",
      "Epoch 03320: reducing learning rate of group 0 to 6.1860e-05.\n",
      "Epoch 03326: reducing learning rate of group 0 to 6.1241e-05.\n",
      "Epoch 03332: reducing learning rate of group 0 to 6.0629e-05.\n",
      "Epoch 03338: reducing learning rate of group 0 to 6.0022e-05.\n",
      "Epoch 03344: reducing learning rate of group 0 to 5.9422e-05.\n",
      "Epoch 03350: reducing learning rate of group 0 to 5.8828e-05.\n",
      "Epoch 03356: reducing learning rate of group 0 to 5.8240e-05.\n",
      "Epoch 03362: reducing learning rate of group 0 to 5.7657e-05.\n",
      "Epoch 03368: reducing learning rate of group 0 to 5.7081e-05.\n",
      "Epoch 03374: reducing learning rate of group 0 to 5.6510e-05.\n",
      "Epoch 03380: reducing learning rate of group 0 to 5.5945e-05.\n",
      "Epoch 03386: reducing learning rate of group 0 to 5.5385e-05.\n",
      "Epoch 03392: reducing learning rate of group 0 to 5.4832e-05.\n",
      "Epoch 03398: reducing learning rate of group 0 to 5.4283e-05.\n",
      "[36m 5s (3400 68%) train loss: 1.7618, test_loss: 1.8821]\n",
      "Whou be oft,\n",
      "that if gove here to hean on beer this so' Rmandi.\n",
      "\n",
      "EENEL:\n",
      "Ckingens.\n",
      "\n",
      "DUT SClromen:\n",
      "T am  \n",
      "\n",
      "Epoch 03404: reducing learning rate of group 0 to 5.3740e-05.\n",
      "Epoch 03410: reducing learning rate of group 0 to 5.3203e-05.\n",
      "Epoch 03416: reducing learning rate of group 0 to 5.2671e-05.\n",
      "Epoch 03422: reducing learning rate of group 0 to 5.2144e-05.\n",
      "Epoch 03428: reducing learning rate of group 0 to 5.1623e-05.\n",
      "Epoch 03434: reducing learning rate of group 0 to 5.1107e-05.\n",
      "Epoch 03440: reducing learning rate of group 0 to 5.0596e-05.\n",
      "Epoch 03446: reducing learning rate of group 0 to 5.0090e-05.\n",
      "Epoch 03452: reducing learning rate of group 0 to 4.9589e-05.\n",
      "Epoch 03458: reducing learning rate of group 0 to 4.9093e-05.\n",
      "Epoch 03464: reducing learning rate of group 0 to 4.8602e-05.\n",
      "Epoch 03470: reducing learning rate of group 0 to 4.8116e-05.\n",
      "Epoch 03476: reducing learning rate of group 0 to 4.7635e-05.\n",
      "Epoch 03482: reducing learning rate of group 0 to 4.7158e-05.\n",
      "Epoch 03488: reducing learning rate of group 0 to 4.6687e-05.\n",
      "Epoch 03494: reducing learning rate of group 0 to 4.6220e-05.\n",
      "Epoch 03500: reducing learning rate of group 0 to 4.5758e-05.\n",
      "[37m 9s (3500 70%) train loss: 1.7603, test_loss: 1.8782]\n",
      "Whis tore to it come remraintt to her exith with your words hert, by the unkingh other with spTenothen \n",
      "\n",
      "Epoch 03506: reducing learning rate of group 0 to 4.5300e-05.\n",
      "Epoch 03512: reducing learning rate of group 0 to 4.4847e-05.\n",
      "Epoch 03518: reducing learning rate of group 0 to 4.4399e-05.\n",
      "Epoch 03524: reducing learning rate of group 0 to 4.3955e-05.\n",
      "Epoch 03530: reducing learning rate of group 0 to 4.3515e-05.\n",
      "Epoch 03536: reducing learning rate of group 0 to 4.3080e-05.\n",
      "Epoch 03542: reducing learning rate of group 0 to 4.2649e-05.\n",
      "Epoch 03548: reducing learning rate of group 0 to 4.2223e-05.\n",
      "Epoch 03554: reducing learning rate of group 0 to 4.1800e-05.\n",
      "Epoch 03560: reducing learning rate of group 0 to 4.1382e-05.\n",
      "Epoch 03566: reducing learning rate of group 0 to 4.0969e-05.\n",
      "Epoch 03572: reducing learning rate of group 0 to 4.0559e-05.\n",
      "Epoch 03578: reducing learning rate of group 0 to 4.0153e-05.\n",
      "Epoch 03584: reducing learning rate of group 0 to 3.9752e-05.\n",
      "Epoch 03590: reducing learning rate of group 0 to 3.9354e-05.\n",
      "Epoch 03596: reducing learning rate of group 0 to 3.8961e-05.\n",
      "[38m 13s (3600 72%) train loss: 1.7635, test_loss: 1.8820]\n",
      "Whatop have we nee\n",
      "And sible art Noor he dished of thevererN;\n",
      "Have breathe, and thefec in the comO me  \n",
      "\n",
      "Epoch 03602: reducing learning rate of group 0 to 3.8571e-05.\n",
      "Epoch 03611: reducing learning rate of group 0 to 3.8185e-05.\n",
      "Epoch 03617: reducing learning rate of group 0 to 3.7804e-05.\n",
      "Epoch 03623: reducing learning rate of group 0 to 3.7426e-05.\n",
      "Epoch 03629: reducing learning rate of group 0 to 3.7051e-05.\n",
      "Epoch 03635: reducing learning rate of group 0 to 3.6681e-05.\n",
      "Epoch 03641: reducing learning rate of group 0 to 3.6314e-05.\n",
      "Epoch 03647: reducing learning rate of group 0 to 3.5951e-05.\n",
      "Epoch 03653: reducing learning rate of group 0 to 3.5591e-05.\n",
      "Epoch 03659: reducing learning rate of group 0 to 3.5235e-05.\n",
      "Epoch 03665: reducing learning rate of group 0 to 3.4883e-05.\n",
      "Epoch 03671: reducing learning rate of group 0 to 3.4534e-05.\n",
      "Epoch 03677: reducing learning rate of group 0 to 3.4189e-05.\n",
      "Epoch 03683: reducing learning rate of group 0 to 3.3847e-05.\n",
      "Epoch 03689: reducing learning rate of group 0 to 3.3509e-05.\n",
      "Epoch 03695: reducing learning rate of group 0 to 3.3173e-05.\n",
      "[39m 17s (3700 74%) train loss: 1.7627, test_loss: 1.8909]\n",
      "Whee thy nator, dhe enlid the marso that, if my lave the reprsely o:\n",
      "O -will the sinedtamentay and viv \n",
      "\n",
      "Epoch 03701: reducing learning rate of group 0 to 3.2842e-05.\n",
      "Epoch 03707: reducing learning rate of group 0 to 3.2513e-05.\n",
      "Epoch 03713: reducing learning rate of group 0 to 3.2188e-05.\n",
      "Epoch 03719: reducing learning rate of group 0 to 3.1866e-05.\n",
      "Epoch 03725: reducing learning rate of group 0 to 3.1548e-05.\n",
      "Epoch 03731: reducing learning rate of group 0 to 3.1232e-05.\n",
      "Epoch 03737: reducing learning rate of group 0 to 3.0920e-05.\n",
      "Epoch 03743: reducing learning rate of group 0 to 3.0611e-05.\n",
      "Epoch 03749: reducing learning rate of group 0 to 3.0305e-05.\n",
      "Epoch 03755: reducing learning rate of group 0 to 3.0001e-05.\n",
      "Epoch 03761: reducing learning rate of group 0 to 2.9701e-05.\n",
      "Epoch 03767: reducing learning rate of group 0 to 2.9404e-05.\n",
      "Epoch 03773: reducing learning rate of group 0 to 2.9110e-05.\n",
      "Epoch 03779: reducing learning rate of group 0 to 2.8819e-05.\n",
      "Epoch 03785: reducing learning rate of group 0 to 2.8531e-05.\n",
      "Epoch 03791: reducing learning rate of group 0 to 2.8246e-05.\n",
      "Epoch 03797: reducing learning rate of group 0 to 2.7963e-05.\n",
      "[40m 20s (3800 76%) train loss: 1.7566, test_loss: 1.8755]\n",
      "Whan gorather.\n",
      "\n",
      "TLOu, GrgOivons wonde'd it bue you to efelay light\n",
      "And fields but it is our deising ao \n",
      "\n",
      "Epoch 03803: reducing learning rate of group 0 to 2.7684e-05.\n",
      "Epoch 03809: reducing learning rate of group 0 to 2.7407e-05.\n",
      "Epoch 03815: reducing learning rate of group 0 to 2.7133e-05.\n",
      "Epoch 03821: reducing learning rate of group 0 to 2.6861e-05.\n",
      "Epoch 03827: reducing learning rate of group 0 to 2.6593e-05.\n",
      "Epoch 03833: reducing learning rate of group 0 to 2.6327e-05.\n",
      "Epoch 03839: reducing learning rate of group 0 to 2.6064e-05.\n",
      "Epoch 03845: reducing learning rate of group 0 to 2.5803e-05.\n",
      "Epoch 03851: reducing learning rate of group 0 to 2.5545e-05.\n",
      "Epoch 03857: reducing learning rate of group 0 to 2.5290e-05.\n",
      "Epoch 03863: reducing learning rate of group 0 to 2.5037e-05.\n",
      "Epoch 03869: reducing learning rate of group 0 to 2.4786e-05.\n",
      "Epoch 03875: reducing learning rate of group 0 to 2.4538e-05.\n",
      "Epoch 03881: reducing learning rate of group 0 to 2.4293e-05.\n",
      "Epoch 03887: reducing learning rate of group 0 to 2.4050e-05.\n",
      "Epoch 03893: reducing learning rate of group 0 to 2.3810e-05.\n",
      "Epoch 03899: reducing learning rate of group 0 to 2.3571e-05.\n",
      "[41m 24s (3900 78%) train loss: 1.7627, test_loss: 1.8923]\n",
      "Whimele, I hath for him frinen in these isten the Borian: to prooant to bore, and me to earing, then t \n",
      "\n",
      "Epoch 03905: reducing learning rate of group 0 to 2.3336e-05.\n",
      "Epoch 03911: reducing learning rate of group 0 to 2.3102e-05.\n",
      "Epoch 03917: reducing learning rate of group 0 to 2.2871e-05.\n",
      "Epoch 03923: reducing learning rate of group 0 to 2.2643e-05.\n",
      "Epoch 03929: reducing learning rate of group 0 to 2.2416e-05.\n",
      "Epoch 03935: reducing learning rate of group 0 to 2.2192e-05.\n",
      "Epoch 03941: reducing learning rate of group 0 to 2.1970e-05.\n",
      "Epoch 03947: reducing learning rate of group 0 to 2.1750e-05.\n",
      "Epoch 03953: reducing learning rate of group 0 to 2.1533e-05.\n",
      "Epoch 03959: reducing learning rate of group 0 to 2.1318e-05.\n",
      "Epoch 03965: reducing learning rate of group 0 to 2.1104e-05.\n",
      "Epoch 03971: reducing learning rate of group 0 to 2.0893e-05.\n",
      "Epoch 03977: reducing learning rate of group 0 to 2.0684e-05.\n",
      "Epoch 03983: reducing learning rate of group 0 to 2.0478e-05.\n",
      "Epoch 03989: reducing learning rate of group 0 to 2.0273e-05.\n",
      "Epoch 03995: reducing learning rate of group 0 to 2.0070e-05.\n",
      "[42m 28s (4000 80%) train loss: 1.7656, test_loss: 1.8691]\n",
      "Whis im theintsfort,\n",
      "And there's I more not saich your hoost the weart-iod with my pace,\n",
      "That the, my  \n",
      "\n",
      "Epoch 04001: reducing learning rate of group 0 to 1.9869e-05.\n",
      "Epoch 04007: reducing learning rate of group 0 to 1.9671e-05.\n",
      "Epoch 04013: reducing learning rate of group 0 to 1.9474e-05.\n",
      "Epoch 04019: reducing learning rate of group 0 to 1.9279e-05.\n",
      "Epoch 04025: reducing learning rate of group 0 to 1.9086e-05.\n",
      "Epoch 04031: reducing learning rate of group 0 to 1.8896e-05.\n",
      "Epoch 04037: reducing learning rate of group 0 to 1.8707e-05.\n",
      "Epoch 04043: reducing learning rate of group 0 to 1.8520e-05.\n",
      "Epoch 04049: reducing learning rate of group 0 to 1.8334e-05.\n",
      "Epoch 04055: reducing learning rate of group 0 to 1.8151e-05.\n",
      "Epoch 04061: reducing learning rate of group 0 to 1.7970e-05.\n",
      "Epoch 04067: reducing learning rate of group 0 to 1.7790e-05.\n",
      "Epoch 04073: reducing learning rate of group 0 to 1.7612e-05.\n",
      "Epoch 04079: reducing learning rate of group 0 to 1.7436e-05.\n",
      "Epoch 04085: reducing learning rate of group 0 to 1.7261e-05.\n",
      "Epoch 04091: reducing learning rate of group 0 to 1.7089e-05.\n",
      "Epoch 04097: reducing learning rate of group 0 to 1.6918e-05.\n",
      "[43m 30s (4100 82%) train loss: 1.7560, test_loss: 1.8797]\n",
      "What lave will don, I at.\n",
      "\n",
      "hOn c,\n",
      "Thion thUong sweebents the pringering hirs with to is my sloues the  \n",
      "\n",
      "Epoch 04103: reducing learning rate of group 0 to 1.6749e-05.\n",
      "Epoch 04109: reducing learning rate of group 0 to 1.6581e-05.\n",
      "Epoch 04115: reducing learning rate of group 0 to 1.6416e-05.\n",
      "Epoch 04121: reducing learning rate of group 0 to 1.6251e-05.\n",
      "Epoch 04127: reducing learning rate of group 0 to 1.6089e-05.\n",
      "Epoch 04133: reducing learning rate of group 0 to 1.5928e-05.\n",
      "Epoch 04139: reducing learning rate of group 0 to 1.5769e-05.\n",
      "Epoch 04145: reducing learning rate of group 0 to 1.5611e-05.\n",
      "Epoch 04151: reducing learning rate of group 0 to 1.5455e-05.\n",
      "Epoch 04157: reducing learning rate of group 0 to 1.5300e-05.\n",
      "Epoch 04163: reducing learning rate of group 0 to 1.5147e-05.\n",
      "Epoch 04169: reducing learning rate of group 0 to 1.4996e-05.\n",
      "Epoch 04175: reducing learning rate of group 0 to 1.4846e-05.\n",
      "Epoch 04181: reducing learning rate of group 0 to 1.4697e-05.\n",
      "Epoch 04187: reducing learning rate of group 0 to 1.4550e-05.\n",
      "Epoch 04193: reducing learning rate of group 0 to 1.4405e-05.\n",
      "Epoch 04199: reducing learning rate of group 0 to 1.4261e-05.\n",
      "[44m 33s (4200 84%) train loss: 1.7691, test_loss: 1.8926]\n",
      "Wherered do that find\n",
      "The master to hatore the dare the onour to the preath a ins comah of battle from \n",
      "\n",
      "Epoch 04205: reducing learning rate of group 0 to 1.4118e-05.\n",
      "Epoch 04211: reducing learning rate of group 0 to 1.3977e-05.\n",
      "Epoch 04217: reducing learning rate of group 0 to 1.3837e-05.\n",
      "Epoch 04223: reducing learning rate of group 0 to 1.3699e-05.\n",
      "Epoch 04229: reducing learning rate of group 0 to 1.3562e-05.\n",
      "Epoch 04235: reducing learning rate of group 0 to 1.3426e-05.\n",
      "Epoch 04246: reducing learning rate of group 0 to 1.3292e-05.\n",
      "Epoch 04252: reducing learning rate of group 0 to 1.3159e-05.\n",
      "Epoch 04258: reducing learning rate of group 0 to 1.3028e-05.\n",
      "Epoch 04264: reducing learning rate of group 0 to 1.2897e-05.\n",
      "Epoch 04270: reducing learning rate of group 0 to 1.2768e-05.\n",
      "Epoch 04276: reducing learning rate of group 0 to 1.2641e-05.\n",
      "Epoch 04282: reducing learning rate of group 0 to 1.2514e-05.\n",
      "Epoch 04288: reducing learning rate of group 0 to 1.2389e-05.\n",
      "Epoch 04294: reducing learning rate of group 0 to 1.2265e-05.\n",
      "Epoch 04300: reducing learning rate of group 0 to 1.2143e-05.\n",
      "[45m 35s (4300 86%) train loss: 1.7550, test_loss: 1.8713]\n",
      "Wheured: thou nognt shation.\n",
      "\n",
      "LARgAr:\n",
      "We ha mase thougeos cousint,s and your sirrans of the pardher me \n",
      "\n",
      "Epoch 04306: reducing learning rate of group 0 to 1.2021e-05.\n",
      "Epoch 04312: reducing learning rate of group 0 to 1.1901e-05.\n",
      "Epoch 04318: reducing learning rate of group 0 to 1.1782e-05.\n",
      "Epoch 04324: reducing learning rate of group 0 to 1.1664e-05.\n",
      "Epoch 04330: reducing learning rate of group 0 to 1.1547e-05.\n",
      "Epoch 04336: reducing learning rate of group 0 to 1.1432e-05.\n",
      "Epoch 04342: reducing learning rate of group 0 to 1.1318e-05.\n",
      "Epoch 04348: reducing learning rate of group 0 to 1.1204e-05.\n",
      "Epoch 04354: reducing learning rate of group 0 to 1.1092e-05.\n",
      "Epoch 04360: reducing learning rate of group 0 to 1.0982e-05.\n",
      "Epoch 04366: reducing learning rate of group 0 to 1.0872e-05.\n",
      "Epoch 04372: reducing learning rate of group 0 to 1.0763e-05.\n",
      "Epoch 04378: reducing learning rate of group 0 to 1.0655e-05.\n",
      "Epoch 04384: reducing learning rate of group 0 to 1.0549e-05.\n",
      "Epoch 04390: reducing learning rate of group 0 to 1.0443e-05.\n",
      "Epoch 04396: reducing learning rate of group 0 to 1.0339e-05.\n",
      "[46m 39s (4400 88%) train loss: 1.7654, test_loss: 1.8692]\n",
      "Why haWt dost a like of the yourgay m,\n",
      "Aindintereds thee fears well theme in bores with they hsolepre, \n",
      "\n",
      "Epoch 04404: reducing learning rate of group 0 to 1.0235e-05.\n",
      "Epoch 04410: reducing learning rate of group 0 to 1.0133e-05.\n",
      "Epoch 04416: reducing learning rate of group 0 to 1.0032e-05.\n",
      "Epoch 04422: reducing learning rate of group 0 to 9.9315e-06.\n",
      "Epoch 04428: reducing learning rate of group 0 to 9.8322e-06.\n",
      "Epoch 04434: reducing learning rate of group 0 to 9.7338e-06.\n",
      "Epoch 04440: reducing learning rate of group 0 to 9.6365e-06.\n",
      "Epoch 04446: reducing learning rate of group 0 to 9.5401e-06.\n",
      "Epoch 04452: reducing learning rate of group 0 to 9.4447e-06.\n",
      "Epoch 04458: reducing learning rate of group 0 to 9.3503e-06.\n",
      "Epoch 04464: reducing learning rate of group 0 to 9.2568e-06.\n",
      "Epoch 04470: reducing learning rate of group 0 to 9.1642e-06.\n",
      "Epoch 04476: reducing learning rate of group 0 to 9.0726e-06.\n",
      "Epoch 04482: reducing learning rate of group 0 to 8.9819e-06.\n",
      "Epoch 04488: reducing learning rate of group 0 to 8.8920e-06.\n",
      "Epoch 04494: reducing learning rate of group 0 to 8.8031e-06.\n",
      "Epoch 04500: reducing learning rate of group 0 to 8.7151e-06.\n",
      "[47m 42s (4500 90%) train loss: 1.7594, test_loss: 1.8964]\n",
      "Whis dound to the m, retes the well of hen the thief,\n",
      "But dost use, being of his ore on with me not ca \n",
      "\n",
      "Epoch 04506: reducing learning rate of group 0 to 8.6279e-06.\n",
      "Epoch 04512: reducing learning rate of group 0 to 8.5417e-06.\n",
      "Epoch 04518: reducing learning rate of group 0 to 8.4562e-06.\n",
      "Epoch 04524: reducing learning rate of group 0 to 8.3717e-06.\n",
      "Epoch 04530: reducing learning rate of group 0 to 8.2880e-06.\n",
      "Epoch 04536: reducing learning rate of group 0 to 8.2051e-06.\n",
      "Epoch 04542: reducing learning rate of group 0 to 8.1230e-06.\n",
      "Epoch 04548: reducing learning rate of group 0 to 8.0418e-06.\n",
      "Epoch 04554: reducing learning rate of group 0 to 7.9614e-06.\n",
      "Epoch 04560: reducing learning rate of group 0 to 7.8818e-06.\n",
      "Epoch 04566: reducing learning rate of group 0 to 7.8029e-06.\n",
      "Epoch 04572: reducing learning rate of group 0 to 7.7249e-06.\n",
      "Epoch 04578: reducing learning rate of group 0 to 7.6477e-06.\n",
      "Epoch 04584: reducing learning rate of group 0 to 7.5712e-06.\n",
      "Epoch 04590: reducing learning rate of group 0 to 7.4955e-06.\n",
      "Epoch 04596: reducing learning rate of group 0 to 7.4205e-06.\n",
      "[48m 45s (4600 92%) train loss: 1.7619, test_loss: 1.8824]\n",
      "Which ertel it in the were their raUif'tess nse agamed and ritherpe or an well.\n",
      "\n",
      "lond ehere's to her-k \n",
      "\n",
      "Epoch 04602: reducing learning rate of group 0 to 7.3463e-06.\n",
      "Epoch 04608: reducing learning rate of group 0 to 7.2729e-06.\n",
      "Epoch 04614: reducing learning rate of group 0 to 7.2001e-06.\n",
      "Epoch 04620: reducing learning rate of group 0 to 7.1281e-06.\n",
      "Epoch 04626: reducing learning rate of group 0 to 7.0568e-06.\n",
      "Epoch 04632: reducing learning rate of group 0 to 6.9863e-06.\n",
      "Epoch 04638: reducing learning rate of group 0 to 6.9164e-06.\n",
      "Epoch 04644: reducing learning rate of group 0 to 6.8472e-06.\n",
      "Epoch 04650: reducing learning rate of group 0 to 6.7788e-06.\n",
      "Epoch 04656: reducing learning rate of group 0 to 6.7110e-06.\n",
      "Epoch 04662: reducing learning rate of group 0 to 6.6439e-06.\n",
      "Epoch 04668: reducing learning rate of group 0 to 6.5774e-06.\n",
      "Epoch 04674: reducing learning rate of group 0 to 6.5117e-06.\n",
      "Epoch 04680: reducing learning rate of group 0 to 6.4465e-06.\n",
      "Epoch 04686: reducing learning rate of group 0 to 6.3821e-06.\n",
      "Epoch 04692: reducing learning rate of group 0 to 6.3183e-06.\n",
      "Epoch 04698: reducing learning rate of group 0 to 6.2551e-06.\n",
      "[49m 48s (4700 94%) train loss: 1.7620, test_loss: 1.8680]\n",
      "What be Casse,\n",
      "What if mho off rust in himed to the is in my roun himd\n",
      "Whisere the tress and thos ase  \n",
      "\n",
      "Epoch 04704: reducing learning rate of group 0 to 6.1925e-06.\n",
      "Epoch 04710: reducing learning rate of group 0 to 6.1306e-06.\n",
      "Epoch 04716: reducing learning rate of group 0 to 6.0693e-06.\n",
      "Epoch 04722: reducing learning rate of group 0 to 6.0086e-06.\n",
      "Epoch 04728: reducing learning rate of group 0 to 5.9485e-06.\n",
      "Epoch 04734: reducing learning rate of group 0 to 5.8890e-06.\n",
      "Epoch 04740: reducing learning rate of group 0 to 5.8301e-06.\n",
      "Epoch 04746: reducing learning rate of group 0 to 5.7718e-06.\n",
      "Epoch 04752: reducing learning rate of group 0 to 5.7141e-06.\n",
      "Epoch 04758: reducing learning rate of group 0 to 5.6570e-06.\n",
      "Epoch 04764: reducing learning rate of group 0 to 5.6004e-06.\n",
      "Epoch 04770: reducing learning rate of group 0 to 5.5444e-06.\n",
      "Epoch 04776: reducing learning rate of group 0 to 5.4890e-06.\n",
      "Epoch 04782: reducing learning rate of group 0 to 5.4341e-06.\n",
      "Epoch 04788: reducing learning rate of group 0 to 5.3797e-06.\n",
      "Epoch 04794: reducing learning rate of group 0 to 5.3259e-06.\n",
      "Epoch 04800: reducing learning rate of group 0 to 5.2727e-06.\n",
      "[50m 51s (4800 96%) train loss: 1.7465, test_loss: 1.8844]\n",
      "While the sone seame beimn vide\n",
      "For done, and worse of her poir renown and proy, and pontaties out to  \n",
      "\n",
      "Epoch 04806: reducing learning rate of group 0 to 5.2199e-06.\n",
      "Epoch 04812: reducing learning rate of group 0 to 5.1678e-06.\n",
      "Epoch 04818: reducing learning rate of group 0 to 5.1161e-06.\n",
      "Epoch 04824: reducing learning rate of group 0 to 5.0649e-06.\n",
      "Epoch 04830: reducing learning rate of group 0 to 5.0143e-06.\n",
      "Epoch 04836: reducing learning rate of group 0 to 4.9641e-06.\n",
      "Epoch 04842: reducing learning rate of group 0 to 4.9145e-06.\n",
      "Epoch 04848: reducing learning rate of group 0 to 4.8653e-06.\n",
      "Epoch 04854: reducing learning rate of group 0 to 4.8167e-06.\n",
      "Epoch 04860: reducing learning rate of group 0 to 4.7685e-06.\n",
      "Epoch 04866: reducing learning rate of group 0 to 4.7208e-06.\n",
      "Epoch 04872: reducing learning rate of group 0 to 4.6736e-06.\n",
      "Epoch 04878: reducing learning rate of group 0 to 4.6269e-06.\n",
      "Epoch 04884: reducing learning rate of group 0 to 4.5806e-06.\n",
      "Epoch 04890: reducing learning rate of group 0 to 4.5348e-06.\n",
      "Epoch 04896: reducing learning rate of group 0 to 4.4895e-06.\n",
      "[51m 55s (4900 98%) train loss: 1.7601, test_loss: 1.8732]\n",
      "Who more is it ngrow tarke \n",
      "HES.\n",
      "And soul endings the chister to his death of the doystrasces the hava \n",
      "\n",
      "Epoch 04902: reducing learning rate of group 0 to 4.4446e-06.\n",
      "Epoch 04908: reducing learning rate of group 0 to 4.4001e-06.\n",
      "Epoch 04914: reducing learning rate of group 0 to 4.3561e-06.\n",
      "Epoch 04920: reducing learning rate of group 0 to 4.3126e-06.\n",
      "Epoch 04926: reducing learning rate of group 0 to 4.2694e-06.\n",
      "Epoch 04932: reducing learning rate of group 0 to 4.2267e-06.\n",
      "Epoch 04938: reducing learning rate of group 0 to 4.1845e-06.\n",
      "Epoch 04944: reducing learning rate of group 0 to 4.1426e-06.\n",
      "Epoch 04950: reducing learning rate of group 0 to 4.1012e-06.\n",
      "Epoch 04956: reducing learning rate of group 0 to 4.0602e-06.\n",
      "Epoch 04962: reducing learning rate of group 0 to 4.0196e-06.\n",
      "Epoch 04968: reducing learning rate of group 0 to 3.9794e-06.\n",
      "Epoch 04974: reducing learning rate of group 0 to 3.9396e-06.\n",
      "Epoch 04980: reducing learning rate of group 0 to 3.9002e-06.\n",
      "Epoch 04986: reducing learning rate of group 0 to 3.8612e-06.\n",
      "Epoch 04992: reducing learning rate of group 0 to 3.8226e-06.\n",
      "Epoch 04998: reducing learning rate of group 0 to 3.7844e-06.\n",
      "[53m 0s (5000 100%) train loss: 1.7507, test_loss: 1.8869]\n",
      "Wher .\n",
      "rain,\n",
      "And then the to?\n",
      "\n",
      "LARmw:\n",
      "You true ament-c\n",
      "a dstlate her mine that dDearur her your what h \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(rnn_optimizer, mode='min', factor=0.99, patience=5, verbose=True)\n",
    "#scheduler = CosineAnnealingLR(rnn_optimizer, T_max=10000, eta_min=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "    scheduler.step(test_loss) \n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "cSeHtUV5TJSl"
   },
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-4BvuZrTJSl"
   },
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "JJrIWoYQTJSm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9a9c985ee0>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path = \"rnn_generator.pth\"\n",
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn.load_state_dict(torch.load(saved_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGmIL0-QTJSm"
   },
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "NtVTz3DQTJSm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heather mean and shall mare the best the was the shall the gentle heart of the place the proming of the ere and for your tord uple the see of the so reard that he are the enessen lied the seed the from the dear to his so manger the fare the word in the are the word in the see this the have the seaten to the tord the word's shall not the sear the war, them the with the would man the come to so have the hand of his so farting the for at this son and stand his shall with so man in her his man the heart the come not of the well the pringan shall be so this not the sore the word to the senting so shall have shall have the conders the stand so that can the worst the meat to say, what mare the soul the mate the soul to end the seath the thee the read the singer the shall in the come the way, and he a thee the that the she stard of the words the part and the pords of the with the bods, the have she shall may we fall the more the will soul to the seer that of the pray shall have the soor the \n"
     ]
    }
   ],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000, temperature=0.35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMlBwJK9TJSm"
   },
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EibyvAJVTJSm",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "machine_shape": "hm",
   "name": "",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
